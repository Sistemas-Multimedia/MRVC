% Emacs, this is -*-latex-*-
\title{Motion Compensated Discrete Wavelet Transform (MCDWT)}
\author{Vicente González Ruiz}
%\makeglossaries
\maketitle
\tableofcontents

\section{Intro}
%{{{

\href{https://en.wikipedia.org/wiki/Video}{Video} data contain high
amounts of \mylink{redundancy}{redundancy}, spatial and temporal. For
this reason, most of \mylink{video_compression}{video encoders}
compress an input sequence of
\href{https://en.wikipedia.org/wiki/Digital_image}{images}\footnote{Which
  from a pure mathematical point of view are matrices of
  \href{https://en.wikipedia.org/wiki/Pixel}{pixels}.}, basically, in
two stages: (1) a transform stage in the
\href{https://en.wikipedia.org/wiki/Time_domain}{time} and in the
\href{https://www.quora.com/What-is-spatial-domain-in-image-processing}{spatial}
domains that produces a collection of
\href{https://en.wikipedia.org/wiki/Decorrelation}{uncorrelated}
\href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{coefficients}, and (2) an
\href{https://en.wikipedia.org/wiki/Entropy_encoding}{entropy
  encoding} phase which removes the statistical redundancy that can
still remains after the decorrelation (transform). These coefficients
have two interesting features:
\begin{enumerate}
\item Usually, a \textbf{
  \href{https://vicente-gonzalez-ruiz.github.io/symbol_compression/}{smaller
  entropy}} than the original pixels. This helps to increase
  the \href{https://en.wikipedia.org/wiki/Data_compression_ratio}{compression
    ratio}.
\item A
  \textbf{\href{https://en.wikipedia.org/wiki/Image_resolution}{multiresolution
      representation} (in space and time)} of the visual
  information. This provides the posibility of decoding the sequence
  of images using a smaller resolution (in space and time). This
  feature is known as \mylink{video_compression}{spatial/temporal
    scalability in video}.
\item Usually, \textbf{most of the
  \href{https://en.wikipedia.org/wiki/Energy_(signal_processing)}{energy}}
  (and therefore, generally, most of the information interesting for
  the humans) \textbf{is represented by
    \href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding}{a
      small number of coefficients}}. This helps to improve the
  compression ratio and to prioritize the data in
  \mylink{media_encoding_models}{progressive transmission scenarios}.
\end{enumerate}

\section{The 2D DWT (Distrete Wavelet Transform)}
\href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding/#x1-3100020}{The
  2D DWT (Distrete Wavelet Transform)\footnote{In this document, we will use the term DWT to refeer to
    the 2D DWT.}} is a digital
transform that, applied to an image, performs a spatial decorrelation
and obtains a
\href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding/index.html#x1-3500024}{multiresolution
  (generally dyadic) representation} of such image, conforming a
collection of DWT subbands. The forward transform converts the image
into a set of frequency subbands with coefficients representing
different spatial areas and frequency orientations. There is an
equivalence between DWT subbands and
\href{http://fourier.eng.hmc.edu/e161/lectures/canny/node3.html}{Laplacian
  Pyramids}, and it is quite simple to
\href{https://vicente-gonzalez-ruiz.github.io/pyramids-and-wavelets/}{pass
  from a decomposition\footnote{The structure that forms the subbands
    is called also a decomposition.} representation to a pyramid
  representation and viceversa}.

\subsection{Implementation of the 3 components (color) DWT step}
\href{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/src/DWT.py}{DWT.py}
(see Fig.~\ref{fig:DWT}) implements the forward (DWT) and backward
1-iteration (step) 2D DWT (iDWT) for color images.

\begin{figure}
  \lstinputlisting[firstline=20, lastline=85, language=python, caption=1-iteration DWT
    (\href{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/src/DWT.py}{DWT.py}).]{../src/DWT.py}
  \lstinputlisting[firstline=87, lastline=108, language=python,
    caption=1-iteration iDWT
    (\href{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/src/DWT.py}{DWT.py}).]{../src/DWT.py}
  \caption{Implementation of the color DWT step (forward and
    backward). $L$ and $H$ stands for \emph{low-pass filtering} and
    \emph{high-pass filtering}, respectively.  More information about the
    implementation can be found at
    \href{https://pywavelets.readthedocs.io/en/latest/index.html}{PyWavelets}.}
  \label{fig:DWT}
\end{figure}

\subsection{Multi-level DWT structures}
A dyadic DWT is a recursive use of the
(1-iteration\footnote{\href{https://pywavelets.readthedocs.io/en/latest/index.html}{PyWavelets}
  uses the term ``levels'' for refering to the number of iterations of
  the 1-interation DWT, and athough levels and interations are closely
  related, we will distinguish between iterations and spatial
  resolution levels.}) DWT applied to the $LL$ subband. For example,
if the DWT is applied $k$-times to the $LL$ subband, recursively, a
decomposition of spatial resolution $(k+1)$ levels (SRLs) is generated. For
the sake of simplicity, we will denote the subbands $\{LH^k, HL^k,
HH^k\}$ as only $H^k$, and $LL^k$ as only $L^k$. See
\href{https://nbviewer.jupyter.org/github/Sistemas-Multimedia/MCDWT/blob/master/docs/DWT_vs_LPT.ipynb}{DWT
  versus LPT}.

\subsection{Scalability provided by the 2D DWT}
Depending on the number of subband levels (pyramid levels in the
\mylink{pyramids-and-wavelets}{pyramid representation}), we can
reconstruct an image at different spatial scales (resolutions). For
example, if an image $s_1$ has been transformed using the DWT of 2
iterations, we get:
\begin{itemize}
\item Subband $s_1.L^2$ with the scale 2.
\begin{verbatim}
    +------+
    |      |
Y/4 |  L2  |
    |      |
    +------+
       X/4
\end{verbatim}
\item Subband $s_1.L^1=s_1.L=\text{iDWT}(s_1.L^2, S_1.H^2)$
  ($\text{iDWT}$) with the scale $1$.
\begin{verbatim}
    +-------------+
    |             |
    |             |
Y/2 |     L1      |
    |             | 
    |             |
    +-------------+
          X/2
\end{verbatim}
  
\item $s_1.L^0=\text{iDWT}(s_1.L, s_1.H)$ (the original image) with
  the scale $0$.
\begin{verbatim}
    +---------------------------+
    |                           |
    |                           |
    |                           | 
    |                           |
    |                           |
    |                           |
  Y |            L0             |
    |                           | 
    |                           |
    |                           | 
    |                           | 
    |                           |
    |                           |
    +---------------------------+
                  X
\end{verbatim}
\end{itemize}

\subsection*{Example: A 1-iteration 2D DWT}
An image is transformed, generating four subbands $LL$, $LH$, $HL$ and
$HH$.
\begin{verbatim}
# You must be in the 'src' directory.

# Copy an image (the output directory must be the same as the input one).
rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp

# 1-iteration 2D DWT.
python3 -O DWT.py -p /tmp/ -i 000

# Visualize the subbands.
display -normalize /tmp/LL000.png # <- LL1
display -normalize /tmp/LH000.png # <- LH1
display -normalize /tmp/HL000.png # <- HL1
display -normalize /tmp/HH000.png # <- HH1

# 1-iteration 2D iDWT.
python3 -O DWT.py -b -p /tmp/ -i 000

# Visualize the reconstruction.
python3 ../tools/substract_offset.py -i /tmp/000.png -o /tmp/1.png
display /tmp/1.png

# Show diferences (PyWavelets uses floating point arithmetic).
../tools/show_differences.sh -1 /tmp/000.png -2 ../sequences/stockholm/000.png -o /tmp/diffs.png
display /tmp/diffs.png
\end{verbatim}

\subsection*{Example: A 2-iterations 2D DWT}
\begin{verbatim}
# You must be in the 'src' directory.

# Copy a image (the output directory must be the same than the input one).
rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp

# First iteration.
python3 -O DWT.py -p /tmp/ -i 000

# Second iteration.
python3 -O DWT.py -p /tmp/LL -i 000

# Visualize the subbands.
ls -l /tmp/*.png
display -normalize /tmp/LL000.png # <- LL1  
display -normalize /tmp/LH000.png # <- LH1  
display -normalize /tmp/HL000.png # <- HL1  
display -normalize /tmp/HH000.png # <- HH1  
display -normalize /tmp/LLLL000.png # <- LL2
display -normalize /tmp/LLLH000.png # <- LH2
display -normalize /tmp/LLHL000.png # <- HL2
display -normalize /tmp/LLHH000.png # <- HH2

# Reverse second iteration.
python3 -O DWT.py -b -p /tmp/LL -i 000
display -normalize /tmp/LL000.png

# Reverse first iteration.
python3 -O DWT.py -b -p /tmp/ -i 000
python3 ../tools/substract_offset.py -i /tmp/000.png -o /tmp/1.png
display /tmp/1.png

# Show diferences.
../tools/show_differences.sh -1 /tmp/000.png -2 ../sequences/stockholm/000.png -o /tmp/diffs.png
display /tmp/diffs.png
\end{verbatim}

%}}}

\section{Motion DWT (MDWT)}
The DWT can be applied to a sequence of images by simply transforming
each image of the sequence independently. This is done, for example,
in the \mylink{JPEG2000}{Motion JPEG2000 video compression
  standard}. Notice that only the spatial redundancy is exploited in
MDWT. All the temporal redundancy still remains in the video. The
decomposition structure generated by MDWT is shown in the
Fig~\ref{fig:forward_MDWT}. 1-iteration MDWT inputs a sequence of
images $\{s_i\}$ and outputs a sequence of decompositions $\{S_i\}$,
each one providing 2 SRLs ($\{s_i.L\}$ and $\{s_i.L^0\}=\{s_i\}$) of
the image sequence.

\begin{figure}
  \centering \myfig{graphics/forward_MDWT}{8cm}{800}
  \lstinputlisting[firstline=21, lastline=42, language=python,
    caption=MDWT
    (\href{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/src/MDWT.py}{MDWT.py}).]{../src/MDWT.py}.
  \lstinputlisting[firstline=44, lastline=66, language=python,
    caption=iMDWT
    (\href{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/src/MDWT.py}{MDWT.py}).]{../src/MDWT.py}.
  \caption{Implementation of the MDWT.} %
  \label{fig:forward_MDWT}
\end{figure}

\subsection{Scalability provided by MDWT}
MDWT sequences (of (subband) decompositions) are scalable in space and
in time. Spatial scalability is a direct consequence of the 2D DWT
(the number of SRLs can be generated bycontrolled by the number of
iterations of DWT). On the other hand, it is trivial to observe that
MDWT provides \emph{fully} temporal scalability (we can access to the
images randomly) because each image of the input sequence is
transformed independently.

\subsection*{Example: 1-iteration MDWT}
\begin{verbatim}
# You must be in the 'src' directory.
rm /tmp/*.png

# Create the output directory for the 1st-level decompositions.
yes | cp ../sequences/stockholm/*.png /tmp

# 2D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/ -N 5

# Visualize the subbands.
for i in /tmp/LL00?.png; do convert -normalize $i $i.norm; done
animate /tmp/LL*.norm
for i in /tmp/LH00?.png; do convert -normalize $i $i.norm; done
animate /tmp/LH*.norm
for i in /tmp/HL00?.png; do convert -normalize $i $i.norm; done
animate /tmp/HL*.norm
for i in /tmp/HH00?.png; do convert -normalize $i $i.norm; done
animate /tmp/HH*.norm

# 1-iteration iMDWT.
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png
\end{verbatim}

\subsection*{Example: 2-iterations MDWT}
\begin{verbatim}
# You must be in the 'src' directory.
rm /tmp/*.png

# Create the output directory for the 1st-level decompositions.
yes | cp ../sequences/stockholm/*.png /tmp

# 2-iterations MDWT.
python3 -O MDWT.py -p /tmp/ -N 5
python3 -O MDWT.py -p /tmp/LL -N 5

# 2-iterations iMDWT.
python3 -O MDWT.py -b -p /tmp/LL -N 5
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png
\end{verbatim}

%}}}

\subsection*{Example: Interpolating with the 2D DWT}
If we apply an inverse transform to the original images, supposing
that they are the low-frequency subbands of the input decomposition,
and that the high-frequency subbands are zero, we can interpolate each
image of the original sequence by a factor of $2$ in each direction.

\begin{verbatim}
# You must be in the 'src' directory.
rm /tmp/*.png

# Copy the current images as LL subbands.
rm /tmp/*.png
for i in {0..4}; do
  image_number=$(printf "%03d" $i)
  cp ../sequences/stockholm/$image_number.png /tmp/LL$image_number.png
done

# Apply the inverse DWT.
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Notice that the dynamic range of the sequence could be changed (the
# images could appear brighter of darker). This is consequence of the
# relative gain of the LL subband in respect of the rest of subbands.
rm /tmp/*.norm
for i in /tmp/???.png; do convert -normalize $i $i.norm; done
animate /tmp/*.png.norm
\end{verbatim}

\section{Video transform alternatives}
%{{{

To remove both, spatial and temporal redundancies, two different
alternatives are available:
\begin{enumerate}
\item In a \textbf{t+2D transform}, the video is first
  \href{https://en.wikipedia.org/wiki/Digital_filter\#Analysis_techniques}{analyzed}
  (transformed) over the time domain and next, over the space domain.
\item A \textbf{2D+t transform} does just the opposite.
\end{enumerate}
Each choice has a number of \emph{pros} and \emph{cons}. For example,
in a t+2D transform we can apply directly any image predictor based on
\href{https://en.wikipedia.org/wiki/Motion_estimation}{motion
  estimation} because the input is a normal video. However, if we
implement a 2D+t transform, the input to the motion estimator is a
sequence decompositions.
\href{http://www.polyvalens.com/blog/wavelets/theory}{The overwhelming
  majority of DWT's} are not
\href{http://www.polyvalens.com/blog/wavelets/theory}{shift
  invariant}, which basically means that, exactly the same object
placed in two different images in different positions will generate
different wavelet coefficients, even if the displacement is an integer
number of pixels and therefore, the pixels of the object in both
images are identical.  Therefore, motion estimators which compare
coefficient values will not work with accuracy on the decomposition
domain. On the other hand, if we want to provide true spatial
scalability (processing only those
\href{https://www.tutorialspoint.com/dip/spatial_resolution.htm}{spatial
  resolutions} (scales) necessary to get a spatially scaled of our
video), a t+2D transformed video presents some drawbacks:

\begin{enumerate}
\item The perfect reconstruction of the original images only is
  possible of the t stage is identical at both, the
  \href{https://en.wikipedia.org/wiki/Encoder}{encoder} and the
  \href{https://en.wikipedia.org/wiki/Decoder}{decoder}. Therefore, if
  the encoder applies t at the full resolution (scale 0), the decoder
  must also apply t at full resolution, even if the resolution of the
  reconstructed images were smaller. This could be unfeasible for
  receivers with low computational resourcers.
\item Perfect reconstruction can be sacrified using a quantized
  version of the motion information, but the rate/distortion (R/D)
  tradeoff will be worst. In this case, also, the decoder would be
  wasting motion information.
\item This last problem (the waste of motion information) could be
  avoided if the motion data were encoded in a progressive
  representation ( suitable for working in a different spatial
  resolution of the images). Unfortunately, progressive
  representations usually need more data than non-progressive ones.
\end{enumerate}

Finally, it is important to realize that the presence of the motion
data in the code-stream introduces also complexity into the decoding
process because, in a quality scalable scenario, for example, it is
not trivial (specially when non-linear systems such as those based on
\mylink{video_compression}{ME} are involved) to decide how to
interleave the motion and the texture information in a code-stream
that can be decoded partially, depending on the avaliable bandwidth.

%}}}

% Subband Motion Compensated Temporal Filtering (SMCTF) 
\section{Motion Compensated Discrete Wavelet Transform (MCDWT)}
MCDWT is a 2D+t transform. The 2D stage is 1-iteration MDWT, and it is
applied first. The t stage is a $T$-iterations 1D
\href{https://en.wikipedia.org/wiki/Motion_compensation}{Motion
  Compensated (MC)} DWT, which removes the temporal redundancy between
adjacent $H$ subbands. The number of iterations $T$ determines the
\href{https://en.wikipedia.org/wiki/Group_of_pictures}{GOP} size
\begin{equation}
  G=2^T.
  \label{eq:GOP_size}
\end{equation}

A special characteristic of MCDWT is that the motion information is
not transmitted from the encoder to the decoder, which must replicate
it or at least, predict it. This is possible because to estimate de
motion at the encoder, only the information that it is accesible by
the decoder is used.

\subsection{The butterfly}
MCDWT butterly inputs three decompositions $a=\{a.L, a.H\}$, $b=\{b.L,
b.H\}$ and $c=\{c.L, c.H\}$, and outputs a residue subband
$\tilde{b}.H$, which replaces to $b.H$ in the original $b$
decomposition. Therefore, after the use of the bufferfly,
we get $a$ (an
\href{https://en.wikipedia.org/wiki/Video_compression_picture_types}{intra-coded
  ``I'' image}), $\tilde{b}$ (a
\href{https://en.wikipedia.org/wiki/Video_compression_picture_types}{bidirectionally
  predicted ``B'' image}\footnote{$\tilde{b}$ can be considered a
  B-type image. However, notice that only the high-frequency
  information of $\tilde{b}$ has been compensated with the
  high-freq. information of the neighbor images. In a normal B image,
  all the frequencies are compensated.}) and $c$ (another intra-coded
``I'' image). This replacement is fully reversible because the forward
transform uses only the information that the inverse transform will
have access to. Notice that the
\href{http://www.vtvt.ece.vt.edu/research/references/video/DCT_Video_Compression/Zhang92a.pdf}{pyramid
  domain} (which is invariant to the pixels displacements) has been
used to estimate and compensate the $H$ subbands.

\begin{figure}
  \centering \myfig{graphics/forward_butterfly}{12cm}{1200}
  \lstinputlisting[firstline=23, lastline=49, language=python,
    caption={MCDWT butterfly
      (\href{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/src/MCDWT.py}{MCDWT.py}).}]{../src/MCDWT.py}
  \lstinputlisting[firstline=51, lastline=77, language=python,
    caption={Inverse MCDWT butterfly
      (\href{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/src/MCDWT.py}{MCDWT.py}).}]{../src/MCDWT.py}
  \lstinputlisting[firstline=4, lastline=9, language=python,
    caption={Computation of the prediction
      image (\href{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/src/simple_average.py}{simple\_average.py}).}]{../src/simple_average.py}
  \caption{MCDWT butterflies.}
  \label{fig:forward_butterfly}
\end{figure}

The ME projections for $[b.L]$ are performed with $[a.L]$ and $[c.L]$
instead of $a$ and $c$ (that are available at the decoder) because, in
$a$ and $c$ there is high-frequency information that can dificult the
computation of the projections $[a.L]\rightarrow [b.L]$ and
$[c.L]\rightarrow [b.L]$.

%\subsection{The MCDWT step}

%\begin{figure}
%  \centering
%  \svg{forward_MCDWT_step}{1200}
%  \caption{MCDWT forward step.}
%  \label{fig:forward_MCDWT_step}
%\end{figure}
%\subsection{Forward and backward (inverse) transform}

\subsection{The transform}
The $T$-iterations MCDWT is the result of applying the MCDWT butterfly
to all the images of $T+1$ different temporal scales (see
Fig.~\ref{fig:MCDWT}). These scales are defined by the images with
number multiple of $2^t$, where $1\leq t\leq T$ is the current
iteration.

\begin{figure}
  \centering
  \myfig{graphics/temporal_decorrelation}{12cm}{1200}
  \lstinputlisting[firstline=79, lastline=131, language=python, caption={MCDWT (\href{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/src/MCDWT.py}{MCDWT.py}).}]{../src/MCDWT.py}
  \lstinputlisting[firstline=133, lastline=174, language=python, caption={iMCDWT (\href{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/src/MCDWT.py}{MCDWT.py}).}]{../src/MCDWT.py}
  \caption{Implementation of the MCDWT.}
  \label{fig:MCDWT}
\end{figure}

Notice that, if $N=G+1$ (where $G$ is the GOP size, see
Eq.~\ref{eq:GOP_size}), except for the first and the last image of the
sequence, all the $H$-subbands are temporally decorrelated.

\subsection*{Example: 1-iteration MDWT + 1-iteration MCDWT}
In this example, a sequence of five images is transformed first with
the (1-iteration) MDWT, producing 2 SRLs by image. Next, the $H$
subbands of the odd images (images 1 and 3) are motion compensated
using the (1-iteration) MCDWT.

\begin{verbatim}
predictor=1
iterations=1

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/*.png /tmp

# 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/

# Show the length of the subbands.
for i in /tmp/LL???.png; do ls -l $i; done
for i in /tmp/LH???.png; do ls -l $i; done
for i in /tmp/HL???.png; do ls -l $i; done
for i in /tmp/HH???.png; do ls -l $i; done

# 1-iteration MCDWT.
python3 -O MCDWT.py -P $predictor -p /tmp/ -T $iterations

# Show the length of the subbands.
for i in /tmp/LL???.png; do ls -l $i; done
for i in /tmp/LH???.png; do ls -l $i; done
for i in /tmp/HL???.png; do ls -l $i; done
for i in /tmp/HH???.png; do ls -l $i; done

# Has changed in length any of them? Remember that a change in lenght
# implies a change in content.

# Lets recover the original sequence ...
rm /tmp/???.png

# 1-iteration iMCDWT.
python3 -O MCDWT.py -P $predictor -b -p /tmp/ -T $iterations

# 1-iteration iMDWT.
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png
\end{verbatim}

\subsection*{Example: 1-iteration MDWT + 2-iterations MCDWT}
This example is similar to the previous, but now the number of
iterations of MCDWT is $T=2$. Now, the second iteration of MCDWT
compensates also the $H$ subband of the image 2. This example matches
exactly with the described in the Fig~\ref{fig:MCDWT}. Notice that the
GOP size is $4$ ($3$ TRLs) and the number of SRLs is $2$.

\begin{verbatim}
predictor=1

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/*.png /tmp

# 1D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/

# Save a copy of the MDWT subbands (for comparing later).
rm -rf /tmp/tmp
mkdir /tmp/tmp
cp /tmp/*.png /tmp/tmp

# 1-iteration MCDWT.
python3 -O MCDWT.py -p /tmp/ -T 1

# Is the content of the MCDWT subbands different to the MDWT's output?
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/LL$ii.png /tmp/tmp/LL$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/LH$ii.png /tmp/tmp/LH$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/HL$ii.png /tmp/tmp/HL$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/HH$ii.png /tmp/tmp/HH$ii.png;echo; done

rm /tmp/*.png
cp ../sequences/stockholm/*.png /tmp

# 1D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/

# Save a copy of the MDWT subbands (for comparing later).
rm -rf /tmp/tmp
mkdir /tmp/tmp
cp /tmp/*.png /tmp/tmp

# 2-iterations MCDWT.
python3 -O MCDWT.py -p /tmp/ -T 2

# Is the content of the MCDWT subbands different to the MDWT's output?
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/LL$ii.png /tmp/tmp/LL$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/LH$ii.png /tmp/tmp/LH$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/HL$ii.png /tmp/tmp/HL$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/HH$ii.png /tmp/tmp/HH$ii.png;echo; done

# Comparing LH visually.
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/tmp/LH$ii.png /tmp/tmp/MDWT_LH$ii.png; done
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/LH$ii.png /tmp/MCDWT_LH$ii.png; done
animate /tmp/tmp/MDWT_LH00?.png &
animate /tmp/MCDWT_LH00?.png &

# Comparing LH002 numerically.
echo MDWT > /tmp/1
printf "%61s\n" MCDWT > /tmp/2
python3 ../tools/show_statistics.py -i /tmp/tmp/LH002.png >> /tmp/1
python3 ../tools/show_statistics.py -i /tmp/LH002.png >> /tmp/2
paste /tmp/1 /tmp/2

# Comparing HL visually.
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/tmp/HL$ii.png /tmp/tmp/MDWT_HL$ii.png; done
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/HL$ii.png /tmp/MCDWT_HL$ii.png; done
animate /tmp/tmp/MDWT_HL00?.png &
animate /tmp/MCDWT_HL00?.png &

# Comparing HL002 numerically.
echo MDWT > /tmp/1
printf "%61s\n" MCDWT > /tmp/2
python3 ../tools/show_statistics.py -i /tmp/tmp/HL002.png >> /tmp/1
python3 ../tools/show_statistics.py -i /tmp/HL002.png >> /tmp/2
paste /tmp/1 /tmp/2

# Comparing HH visually.
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/tmp/HH$ii.png /tmp/tmp/MDWT_HH$ii.png; done
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/HH$ii.png /tmp/MCDWT_HH$ii.png; done
animate /tmp/tmp/MDWT_HH00?.png &
animate /tmp/MCDWT_HH00?.png &

# Comparing HH002 numerically.
echo MDWT > /tmp/1
printf "%61s\n" MCDWT > /tmp/2
python3 ../tools/show_statistics.py -i /tmp/tmp/HH002.png >> /tmp/1
python3 ../tools/show_statistics.py -i /tmp/HH002.png >> /tmp/2
paste /tmp/1 /tmp/2

# Now we recover the original video.

# 2-iterations iMCDWT.
python3 -O MCDWT.py -P $predictor -b -p /tmp/ -T $iterations

# 1-iteration iMDWT
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png

python3 ../tools/show_statistics.py -i /tmp/diff_002.png
\end{verbatim}

\subsection*{Example: 1-iteration MDWT + 4-iterations MCDWT}
A exaple with $G=2^3=16$.

\begin{verbatim}
# You must be in the 'src' directory.
rm /tmp/*.png
video_file=/tmp/akiyo_cif.y4m
if test -f $video_file; then
  print akiyo exists
else
  wget https://media.xiph.org/video/derf/y4m/akiyo_cif.y4m -O /tmp/akiyo_cif.y4m
fi
mplayer /tmp/akiyo_cif.y4m
ffmpeg -i /tmp/akiyo_cif.y4m -vframes 17 -start_number 0 /tmp/%03d.png

# 1D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/ -N 17

# Save a copy of the MDWT subbands (for comparing later).
rm -rf /tmp/tmp
mkdir /tmp/tmp
cp /tmp/LL*.png /tmp/tmp
cp /tmp/LH*.png /tmp/tmp
cp /tmp/HL*.png /tmp/tmp
cp /tmp/HH*.png /tmp/tmp

# 4-iterations MCDWT.
python3 -O MCDWT.py -p /tmp/ -T 4 -N 17

ls -l /tmp/LH*.png > /tmp/1
ls -l /tmp/tmp/LH*.png > /tmp/2
paste /tmp/1 /tmp/2

ls -l /tmp/HL*.png > /tmp/1
ls -l /tmp/tmp/HL*.png > /tmp/2
paste /tmp/1 /tmp/2

ls -l /tmp/HH*.png > /tmp/1
ls -l /tmp/tmp/HH*.png > /tmp/2
paste /tmp/1 /tmp/2

# Now we recover the original video.

# 4-iterations iMCDWT.
python3 -O MCDWT.py -b -p /tmp/ -T 4 -N 17

# 1-iteration iMDWT
python3 -O MDWT.py -b -p /tmp/ -N 17

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..16}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png

python3 ../tools/show_statistics.py -i /tmp/diff_002.png
\end{verbatim}

\subsection{Scalability provided by MCDWT}
MCDWT preserves the dyadic spatial decomposition generated by MDWT,
and therefore, MCDWT provides the same spatial scalability than MDWT.

Unfortunately, as the rest of video encoders based on MC, MCDWT
reduces the temporal scalability provided by MDWT, allowing only a
dyadic access to the images. For example, after applying each
iteration of the inverse MCDWT, if $T=3$ (the GOP size $G=8$, i.e. $4$
temporal resolution levels (TRLs) have been generated), the images of
the second GOP (GOP 1, considereing that the first GOP (GOP 0) is only
composed by the first image of the sequence) should be reconstructed
in this order:

\begin{verbatim}

GOP0          GOP 1
---- ------------------------
  s0                       s8  <- output of the 3-iterations MCDWT. Provides TRL3.
               s4              <- first iteration of the inverse MCDWT. Provides TRL2.
         s2          s6        <- second iteration. Provides TRL1.
      s1    s3    s5    s7     <- third iteration. Provides TRL0.

\end{verbatim}

\subsection*{Example: 3-iters $\times$ (1-iter MDWT + 2-iters MCDWT)}

\begin{verbatim}
predictor=1
iterations=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

predictor=1
python3 -O MDWT.py  -p /tmp/
python3 -O MCDWT.py -p /tmp/     -P $predictor -T $iterations
python3 -O MDWT.py  -p /tmp/LL
python3 -O MCDWT.py -p /tmp/LL   -P $predictor -T $iterations
python3 -O MDWT.py  -p /tmp/LLLL
python3 -O MCDWT.py -p /tmp/LLLL -P $predictor -T $iterations

rm /tmp/00?.png

python3 -O MCDWT.py -p /tmp/LLLL -P $predictor -b -T $iterations
python3 -O MDWT.py  -p /tmp/LLLL               -b
python3 -O MCDWT.py -p /tmp/LL   -P $predictor -b -T $iterations
python3 -O MDWT.py  -p /tmp/LL                 -b
python3 -O MCDWT.py -p /tmp/     -P $predictor -b -T $iterations
python3 -O MDWT.py  -p /tmp/                   -b

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png
\end{verbatim}

The GOP size is $4$, generating $3$ TRLs in this case. $4$ is the number of
SRLs.

\begin{verbatim}

TRLs:

GOP0      GOP 1
---- -------------
  s0           s4  <- TRL1 U {s0, s4} = TRL2
         s2        <- TRL0 U s2 = TRL1
      s1    s3     <- TRL0

SRLs:

+---+---+-------+---------------+
| 3 | 2 |   1   |       0       |
+---+   |       |               |
|       |       |               |
+-------+       |               |
|               |               |
|               |               |
|               |               |
+---------------+               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
+-------------------------------+
\end{verbatim}

$3\times 4=12$ different possible reconstructions of the video are possible:
\begin{enumerate}
\item s0.3, s4.3.
\item s0.2, s4.2.
\item s0.1, s4.1.
\item s0.0, s4.0.
\item s0.3, s4.3, s2.3.
\item s0.2, s4.2, s2.2.
\item s0.1, s4.1, s2.1.
\item s0.0, s4.0, s2.0.
\item s0.3, s4.3, s2.3, s1.3, s3.3.
\item s0.2, s4.2, s2.2, s1.2, s3.2.
\item s0.1, s4.1, s2.1, s1.1, s3.1.
\item s0.0, s4.0, s2.0, s1.0, s3.0.
\end{enumerate}

\begin{comment}
  \subsection{Temporal interpolation}

Hay que trasladar el lifting a este nuevo diseño. Así se sabrá mejor cómo realizar la interpolación temporal.
  
  Si conseguimos a->c y c->a, siendo a y c dos imágenes adyacentes (000 y 001, por ejemplo), podemos generar la imagen predicha proyectando a usando (a->c)/2 y c usando (c->a)/2. Así conseguimos 2 proyecciones, con las que podemos hacer la media o usar el error de predicción para calcular la proporción de cada proyección.
\begin{verbatim}
predictor=1

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

predictor=1
python3 -O MDWT.py  -p /tmp/
python3 -O MCDWT.py -p /tmp/     -P $predictor -T 1

rm /tmp/00?.png

mv /tmp/LL004.png /tmp/LL008.png
mv /tmp/LL003.png /tmp/LL006.png
mv /tmp/LL002.png /tmp/LL004.png
mv /tmp/LL001.png /tmp/LL002.png
mv /tmp/LH004.png /tmp/LH008.png
mv /tmp/LH003.png /tmp/LH006.png
mv /tmp/LH002.png /tmp/LH004.png
mv /tmp/LH001.png /tmp/LH002.png
mv /tmp/HL004.png /tmp/HL008.png
mv /tmp/HL003.png /tmp/HL006.png
mv /tmp/HL002.png /tmp/HL004.png
mv /tmp/HL001.png /tmp/HL002.png
mv /tmp/HH004.png /tmp/HH008.png
mv /tmp/HH003.png /tmp/HH006.png
mv /tmp/HH002.png /tmp/HH004.png
mv /tmp/HH001.png /tmp/HH002.png

python3 -O MCDWT.py -b -p /tmp/ -N 9 -P $predictor -T 2
python3 -O MDWT.py  -b -p /tmp/ -N 9

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png



predictor=1

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

predictor=1
python3 -O MDWT.py  -p /tmp/
python3 -O MCDWT.py -p /tmp/     -P $predictor -T 1

rm /tmp/00?.png

python3 -O MCDWT.py -p /tmp/     -P $predictor -b -T 2
python3 -O MDWT.py  -p /tmp/                   -b

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png



rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp/LL000.png
cp ../sequences/stockholm/000.png /tmp/LL001.png
cp ../sequences/stockholm/001.png /tmp/LL002.png
cp ../sequences/stockholm/001.png /tmp/LL003.png
cp ../sequences/stockholm/002.png /tmp/LL004.png
cp ../sequences/stockholm/002.png /tmp/LL005.png
cp ../sequences/stockholm/003.png /tmp/LL006.png
cp ../sequences/stockholm/003.png /tmp/LL007.png
cp ../sequences/stockholm/004.png /tmp/LL008.png
cp ../sequences/stockholm/004.png /tmp/LL009.png
python3 -O MCDWT.py -p /tmp/ -N 9 -b
\end{verbatim}        
\end{comment}

\begin{comment}
\subsection{Accuracy of the motion compensation (TO BE IMPLEMENTED)}
The movement of the objects in the scene captured by a video is a
continuous event. Conversely, the digital images taken from such
objects are discrete in the time and the space domains. This means
that, in general, two different positions of an object in two
different images is not going to match with a distance that can be
measured by an integer number of pixels. For example, lets suppose that the content of the image $s_0$ is:
\begin{verbatim}
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
\end{verbatim}
and that the camera has moved, following a parallel movement to the
surface of the object of 1/2 pixels. In this case, the image $s_1$
taken from such object would be:
\begin{verbatim}
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
\end{verbatim}
and it can be seen, it is difficult to determine that the object has
been moved, because the values of the pixels have changed
significantly. However, if we interpolate both images using the
wavelet ``bior3.5'' of PyWavelets, we obtain for the first
image\footnote{\texttt{pywt.idwt([0,0,0,255,0,0,0],None,'bior3.5',
    'per')}}:
\begin{verbatim}
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
\end{verbatim}
and for the second
one\footnote{\texttt{pywt.idwt([0,0,128,128,0,0,0],None,'bior3.5',
    'per')}}:
\begin{verbatim}
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
\end{verbatim}
that is more suitable for determining the sub-pixel accuracy motion.

So, to increase the accuracy of the MC step to 1/2 sub-pixel accuracy
(for example), we can interpolate\footnote{Considering the unkown
  high-frequency information as zero.} all the subbands that are
involved in the MC process by means of a 2-iterations iDWT, generating
$[a.L]^2$, $[c.L]^2$, $[a.H]^2$, $[b.H]^2$ and $[c.H]^2$, and finally,
using a 2-iterations DWT, to obtain $\tilde{b}$.
\end{comment}

%\subsection{(Spatial) Multiresolution}
%Spatial dyadic multiresolution can be obtained by appliying a sequence
%of (MDWT+MCDWT) steps (see Fig.~\ref{fig:multiresolution}).

%\begin{figure}
%  \centering %
%  \myfig{graphics/multiresolution}{10cm}{1200}
%  \caption{MDWT+MCDWT multiresolution procedure. The input to the second
%    iteration of the MDWT+MCDWT transform is the output of a previous
%    (first) iteration MDWT+MCDWT transform. The second iteration is only
%    applied to the scale 1.} %
%  \label{fig:multiresolution}
%\end{figure}

%Spatial dyadic multiresolution can be obtained by appliying a sequence
%of MDWT+MCDWT steps (see Fig.~\ref{fig:multiresolution}).

\section{Adaptive motion compensation based on the distortion of the prediction error}
%{{{

As can be seen in the MCDWT bufferfly (see
Fig.~\ref{fig:forward_butterfly}), both predictions $[b_a.H]$ and
$[b_c.H]$ have the same weight to build the prediction
\begin{equation}
  [\hat{b.H}] = \frac{[b_a.H] + [b_c.H]}{2}.
\end{equation}
This simple computation, that can have very effective for example in
\href{https://biteable.com/blog/tips/video-transitions-effects-examples/}{dissolves}
video transitions, can also be counterproductive when objects appear
only in one of the reference images. For this reason, in this section
a different predictor is proposed, based on the estimated prediction
error generated throughout the ME process ($[\tilde{b}.H]$ in the
Fig.~\ref{fig:forward_butterfly}). To estimate this error, we will
suppose that the prediction error generated for the high frequencies
of an image is proportional to the prediction error generated for the
low frequencies of such image.

Let $[b_a.L]$ the prediction generated for the subband $[b.L]$ using
$[a.L]$ as reference and $[a.L]\rightarrow [b.L]$ as motion, and let
$[b_c.L]$ the prediction generated for $[b.L]$ using $[c.L]$ as
reference and $[c.L]\rightarrow [b.L]$ as motion. We now compute the
prediction errors
\begin{equation}
  \begin{array}{l}
    {[e_a.L]} = [b.L] - [b_a.L]\\
    {[e_c.L]} = [b.L] - [b_c.L].
  \end{array}
\end{equation}

We define the similarity matrices as
\begin{equation}
  \begin{array}{l}
    {[s_a.L]} = \frac{1}{1+{|[e_a.L]|}}\\
    {[s_c.L]} = \frac{1}{1+{|[e_c.L]|}}.    
  \end{array}
  \label{eq:weighted_prediction}
\end{equation}
Notice that, if (for example) the error $[e_a.L]_{x,y}=0$, the
similarity is $[s_a.L]_{x,y}=1$ (the maximum similarity), if the error
is high, the similarity tends to be low (but never $0$).

With this information, that can be recovered by the decoder, the
improved prediction is defined as
\begin{equation}
  [\hat{b.H}] = \frac{[b_a.H][s_a.L]+[s_c.L][b_c.H]}{[s_a.L]+[s_c.L]}.
\end{equation}
The Fig.~\ref{fig:weighted_average} shows an implementation of this
equation.

Notice that, if (for example) $[s_a.L]_{x,y}=1$ and
$[s_c.L]_{x,y}\approx 0$, then
$[\hat{b.H}]_{x,y} \approx [b_a.H]_{x,y}$, and viceversa. If
$[s_a.L]_{x,y}=[s_c.L]_{x,y}$, then
$[\hat{b.H}]_{x,y}=\frac{[b_a.H]_{x,y}+[b_c.H]_{x,y}}{2}$ (even if both similarities are small).

\begin{figure}
  \centering \lstinputlisting[firstline=4, lastline=15,
    language=python, caption={\href{https://github.com/Sistemas-Multimedia/MCDWT/blob/master/src/weighted_average.py}{weighted\_average.py}.}]{../src/weighted_average.py}
  \caption{Computation of the weighted average prediction.}
  \label{fig:weighted_average}
\end{figure}
%}}}


\section{Lossy compression: quantization}
\mylink{quantization}{Quantization} allows to remove information from
signals. We will use it in MCDWT to achieve higher compression ratios,
but at the expense of losing visual information (ideally, the less
relevant information should be removed first).

When the signal cannot be recoverd perfectly, we can use a
\mylink{distortion_metrics}{distortion metric} to find how much error
has been generated.

\subsection*{Example: Effects caused by the total attenuation of the motion compensated (MCed) $H$ subbands}

\begin{verbatim}
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/*.png /tmp


# 1D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/

# 1D 1-iteration MCDWT.
python3 -O MCDWT.py -p /tmp/

# Let's delete the motion compensated subbands and reconstruct the sequence ...
rm -f /tmp/LH001.png
rm -f /tmp/HL001.png
rm -f /tmp/HH001.png
rm -f /tmp/LH002.png
rm -f /tmp/HL002.png
rm -f /tmp/HH002.png
rm -f /tmp/LH003.png
rm -f /tmp/HL003.png
rm -f /tmp/HH003.png

# 1D 1-iteration iMCDWT.
python3 -O MCDWT.py -P $predictor -b -p /tmp/

# 1D 1-iteration iMDWT.
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction.
for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

# Visualization of the differences with the original sequence.
rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png

# Distortion per image

\end{verbatim}

%}}}

%{{{
\subsection*{Example: Effects caused by the quantization of MCed $H$ subbands}
\begin{verbatim}
q_step=32
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCDWT.py -P $predictor -p /tmp/

#python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/quantize.py -i /tmp/LH001.png -o /tmp/LH001.png -q $q_step
#python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/quantize.py -i /tmp/HL001.png -o /tmp/HL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH001.png -o /tmp/HH001.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH002.png -o /tmp/LH002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL002.png -o /tmp/HL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH002.png -o /tmp/HH002.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH003.png -o /tmp/LH003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL003.png -o /tmp/HL003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH003.png -o /tmp/HH003.png -q $q_step

python3 -O MCDWT.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

\subsection*{Example: Quantization of all $H$ subbands (MCed and intra)}
\begin{verbatim}
q_step=32
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCDWT.py -P $predictor -p /tmp/

python3 ../tools/quantize.py -i /tmp/LH000.png -o /tmp/LH000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL000.png -o /tmp/HL000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH000.png -o /tmp/HH000.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH001.png -o /tmp/LH001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL001.png -o /tmp/HL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH001.png -o /tmp/HH001.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH002.png -o /tmp/LH002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL002.png -o /tmp/HL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH002.png -o /tmp/HH002.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH003.png -o /tmp/LH003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL003.png -o /tmp/HL003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH003.png -o /tmp/HH003.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH004.png -o /tmp/LH004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL004.png -o /tmp/HL004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH004.png -o /tmp/HH004.png -q $q_step

python3 -O MCDWT.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

\subsection*{Example: Quantization of all subbands}
\begin{verbatim}
q_step=128
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCDWT.py -P $predictor -p /tmp/

python3 ../tools/quantize.py -i /tmp/LL000.png -o /tmp/LL000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH000.png -o /tmp/LH000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL000.png -o /tmp/HL000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH000.png -o /tmp/HH000.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LL001.png -o /tmp/LL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH001.png -o /tmp/LH001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL001.png -o /tmp/HL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH001.png -o /tmp/HH001.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LL002.png -o /tmp/LL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH002.png -o /tmp/LH002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL002.png -o /tmp/HL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH002.png -o /tmp/HH002.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LL003.png -o /tmp/LL003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH003.png -o /tmp/LH003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL003.png -o /tmp/HL003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH003.png -o /tmp/HH003.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LL004.png -o /tmp/LL004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH004.png -o /tmp/LH004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL004.png -o /tmp/HL004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH004.png -o /tmp/HH004.png -q $q_step

python3 -O MCDWT.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b 

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

%}}}

\begin{comment}

%{{{
\subsection*{Example: Lossy compression of the MC $H$ subbands}
\begin{verbatim}
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCDWT.py -P $predictor -p /tmp/

quality=50
python3 ../tools/compress.py -i /tmp/LH001.png -o /tmp/LH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH001.png -o /tmp/LH001.png -f 32640
python3 ../tools/compress.py -i /tmp/HL001.png -o /tmp/HL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL001.png -o /tmp/HL001.png -f 32640
python3 ../tools/compress.py -i /tmp/HH001.png -o /tmp/HH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH001.png -o /tmp/HH001.png -f 32640

python3 ../tools/compress.py -i /tmp/LH002.png -o /tmp/LH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH002.png -o /tmp/LH002.png -f 32640
python3 ../tools/compress.py -i /tmp/HL002.png -o /tmp/HL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL002.png -o /tmp/HL002.png -f 32640
python3 ../tools/compress.py -i /tmp/HH002.png -o /tmp/HH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH002.png -o /tmp/HH002.png -f 32640

python3 ../tools/compress.py -i /tmp/LH003.png -o /tmp/LH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH003.png -o /tmp/LH003.png -f 32640
python3 ../tools/compress.py -i /tmp/HL003.png -o /tmp/HL003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL003.png -o /tmp/HL003.png -f 32640
python3 ../tools/compress.py -i /tmp/HH003.png -o /tmp/HH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH003.png -o /tmp/HH003.png -f 32640

python3 -O MCDWT.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

\subsection*{Example: Lossy compression of all $H$ subbands}
\begin{verbatim}
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCDWT.py -P $predictor -p /tmp/

quality=10
python3 ../tools/compress.py -i /tmp/LH000.png -o /tmp/LH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH000.png -o /tmp/LH000.png -f 32640
python3 ../tools/compress.py -i /tmp/HL000.png -o /tmp/HL000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL000.png -o /tmp/HL000.png -f 32640
python3 ../tools/compress.py -i /tmp/HH000.png -o /tmp/HH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH000.png -o /tmp/HH000.png -f 32640

python3 ../tools/compress.py -i /tmp/LH001.png -o /tmp/LH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH001.png -o /tmp/LH001.png -f 32640
python3 ../tools/compress.py -i /tmp/HL001.png -o /tmp/HL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL001.png -o /tmp/HL001.png -f 32640
python3 ../tools/compress.py -i /tmp/HH001.png -o /tmp/HH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH001.png -o /tmp/HH001.png -f 32640

python3 ../tools/compress.py -i /tmp/LH002.png -o /tmp/LH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH002.png -o /tmp/LH002.png -f 32640
python3 ../tools/compress.py -i /tmp/HL002.png -o /tmp/HL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL002.png -o /tmp/HL002.png -f 32640
python3 ../tools/compress.py -i /tmp/HH002.png -o /tmp/HH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH002.png -o /tmp/HH002.png -f 32640

python3 ../tools/compress.py -i /tmp/LH003.png -o /tmp/LH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH003.png -o /tmp/LH003.png -f 32640
python3 ../tools/compress.py -i /tmp/HL003.png -o /tmp/HL003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL003.png -o /tmp/HL003.png -f 32640
python3 ../tools/compress.py -i /tmp/HH003.png -o /tmp/HH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH003.png -o /tmp/HH003.png -f 32640

python3 ../tools/compress.py -i /tmp/LH004.png -o /tmp/LH004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH004.png -o /tmp/LH004.png -f 32640
python3 ../tools/compress.py -i /tmp/HL004.png -o /tmp/HL004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL004.png -o /tmp/HL004.png -f 32640
python3 ../tools/compress.py -i /tmp/HH004.png -o /tmp/HH004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH004.png -o /tmp/HH004.png -f 32640

python3 -O MCDWT.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

\subsection*{Example: Lossy compression of all subbands}
\begin{verbatim}
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCDWT.py -P $predictor -p /tmp/

quality=10
python3 ../tools/compress.py -i /tmp/LL000.png -o /tmp/LL000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL000.png -o /tmp/LL000.png -f 32640
python3 ../tools/compress.py -i /tmp/LH000.png -o /tmp/LH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH000.png -o /tmp/LH000.png -f 32640
python3 ../tools/compress.py -i /tmp/HL000.png -o /tmp/HL000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL000.png -o /tmp/HL000.png -f 32640
python3 ../tools/compress.py -i /tmp/HH000.png -o /tmp/HH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH000.png -o /tmp/HH000.png -f 32640

python3 ../tools/compress.py -i /tmp/LL001.png -o /tmp/LL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL001.png -o /tmp/LL001.png -f 32640
python3 ../tools/compress.py -i /tmp/LH001.png -o /tmp/LH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH001.png -o /tmp/LH001.png -f 32640
python3 ../tools/compress.py -i /tmp/HL001.png -o /tmp/HL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL001.png -o /tmp/HL001.png -f 32640
python3 ../tools/compress.py -i /tmp/HH001.png -o /tmp/HH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH001.png -o /tmp/HH001.png -f 32640

python3 ../tools/compress.py -i /tmp/LL002.png -o /tmp/LL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL002.png -o /tmp/LL002.png -f 32640
python3 ../tools/compress.py -i /tmp/LH002.png -o /tmp/LH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH002.png -o /tmp/LH002.png -f 32640
python3 ../tools/compress.py -i /tmp/HL002.png -o /tmp/HL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL002.png -o /tmp/HL002.png -f 32640
python3 ../tools/compress.py -i /tmp/HH002.png -o /tmp/HH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH002.png -o /tmp/HH002.png -f 32640

python3 ../tools/compress.py -i /tmp/LL003.png -o /tmp/LL003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL003.png -o /tmp/LL003.png -f 32640
python3 ../tools/compress.py -i /tmp/LH003.png -o /tmp/LH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH003.png -o /tmp/LH003.png -f 32640
python3 ../tools/compress.py -i /tmp/HL003.png -o /tmp/HL003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL003.png -o /tmp/HL003.png -f 32640
python3 ../tools/compress.py -i /tmp/HH003.png -o /tmp/HH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH003.png -o /tmp/HH003.png -f 32640

python3 ../tools/compress.py -i /tmp/LL004.png -o /tmp/LL004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL004.png -o /tmp/LL004.png -f 32640
python3 ../tools/compress.py -i /tmp/LH004.png -o /tmp/LH004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH004.png -o /tmp/LH004.png -f 32640
python3 ../tools/compress.py -i /tmp/HL004.png -o /tmp/HL004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL004.png -o /tmp/HL004.png -f 32640
python3 ../tools/compress.py -i /tmp/HH004.png -o /tmp/HH004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH004.png -o /tmp/HH004.png -f 32640

python3 -O MCDWT.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

%}}}

\end{comment}

\begin{comment}
\section{MCDWT decomposition}
%{{{

\begin{figure}
  \centering %
  \myfig{graphics/MCDWT_decomposition}{12cm}{1200} %
  \caption{MCDWT($S=1$, $T=3$) decomposition (notice that
    $\text{GOP\_size}=8=2^T$). The arrows indicate image dependencies
    (for example, to decode image 1, images 0 and 2 should have been
    decoded.} %
  \label{fig:MCDWT_decomposition}
\end{figure}

MCDWT($S$, $T$), where $S$ is the number of levels of the spatial
transform and $T$ is the number of levels of the temporal transform,
decomposes a sequence of images into a sequence of subbands organized
in $T+1$ of temporal scales of $S+1$ spatial scales (see
Fig.~\ref{fig:MCDWT_decomposition}).

%}}}
\end{comment}

\begin{comment}
\section{Encoding of B-type H-subbands (experimental)}
%{{{

The subband $\tilde{b}.H$ generated by the MCDWT butterfly (see
Sec.~\ref{sec:butterfly}) is no longer needed for applying the
butterfly to different decompositions and scales. For this reason, we
can compress this subband. As a result, after using the butterfly to
<<<<<<< all the decompositions of the sequence and all the scales, all the $H$
subbands can be compressed, excepting the intra-coded image of each
GOP. Notice that the redundancy exploited in B-type $H$ subbands is
temporal.

A $\tilde{b}.H$ subband can be compressed by sorting its coefficients
by energy and predicting them. Thus, the better the prediction the
higher the compression ratio. To sort the coefficients of subband
$\tilde{b}.H$ by energy without using it (remember that the decoder
does not know this information), we can think that there is a
correlation between the coefficients of $\tilde{b}.H$ and
$\tilde{b}.L$, that is, the prediction error resulting of substracting
to $b.L$ a prediction $\hat{b}.L$ generated with the same motion
information used to built the prediction $\hat{b}.H$. Such idea has
been implemented in the following algorithm:

\begin{enumerate}
\item [1.] Compute the prediction error for the $[b.L]$ subband
\begin{equation}
  [\tilde{b}.L] = [b.L] - [\hat{b}.L]
\end{equation}
where (see Eq.~\ref{eq:weighted_prediction})
\begin{equation}
  [\hat{b}.L] = \frac{[b_a.L][s_a.L]+[s_c.L][b_c.L]}{[s_a.L]+[s_c.L]}.
\end{equation}

\item [2.] Compute the 2D-DWT of subband $L$
  \begin{equation}
    \tilde{b}=\text{DWT}([\tilde{b}.L]).
  \end{equation}

\item [3.] Find the indices for $\tilde{b}.L$ that sorts it in descending order by energy
  \begin{equation}
    \text{indices}=\text{unravel\_index}(\text{argsort}(\text{abs}(\tilde{b}.L),
    b.L.\text{width}, b.L.\text{height})).
  \end{equation}
%\item [3.] For each coefficient $x,y$ in $\tilde{b}.L$:
%  \begin{enumerate}
%  \item [a.] $d_{i=x\times b.\text{width}+y} = (\text{abs}(\tilde{b}_{x,y}), x, y)$
%  \end{enumerate}
%\item [4.] Sort $d$ in desceding order sorted by field 0 /* use the amplitude of luminance */.
\item Go over $\tilde{b}.L$ sending the wavelet coefficients of subbands
  $LH^2$, $HL^2$ and $HH^2$ in descending order by energy.
%\item [5.] Estimate the nearest power of two smaller or equan than the maximum amplitude
%  \begin{equation}
%    \lambda_0 = \lambda = 1 << \text{int}(\log_2(\text{abs}(d_{0}))).
%  \end{equation}
%\item [6.] For each coefficient $i$ in $d$:
%  \begin{enumerate}
%  \item [a.] $x,y = d_i[1:2]$.
%  \item [b.] If any of
%    $1<<\text{int}(\log_2(\tilde{b.H}_{x,y}))==\lambda_0$ then send
%    $\text{sign}(\tilde{b.H}_{x,y})$ for the three (LH, HL and HH)
%    subbands.
%  \item [b.] Send $(\hat{b.H}_{x,y}~\text{bitwise-and}~\lambda)/\lambda$ for
%    the three (LH, HL and HH) subbands.
%  \end{enumerate}
\item [7.] $\lambda >>= 1$.
\item [8.] Goto step 6.
\end{enumerate}

%}}}
\end{comment}

\begin{comment}
  \begin{comment}
    nola
  \end{comment}
\end{comment}

\begin{comment}
\section{Encoding of I-type H-subbands (experimental)}
%{{{

After a $T$-levels MCDWT transform of a sequence, $T+1$ temporal
subbands are generated, and the temporal subband $L^{T+1}$ has a one
I-type subband for each GOP (see
Fig~\ref{fig:MCDWT_decomposition}). Notice that, in a I-type image,
the redundancy exploited to accumulate the visual information in the
$L$ subband (or to remove the visual information in the $H$ subbands)
is spatial.

After using MCDWT, I-type images are decomposed into
the four subbands $LL$ (low-frequencies), $LH$, $HL$ and $HH$
(high-frequencies). Depending on the image content and wavelets used
in the 2D-DWT, $H$ subbands contain a certain amount of \emph{visual
  information}\footnote{Any visual stimulus that provides information
  to humans}. The key here is to reduce as much as possible such
visual information, leaving in the $H$ subbands only \emph{visual
  noise}\footnote{Any visual stimulus that does not provides
  information to humans.} If we achieve this, the $H$ subbands,
expressed in a sign/magnitude representation will have two features:
(1) the probability of finding zeros when we are moving from the least
significant bit-planes fo the most significant ones of the magnitude
of the coefficients will increase (to the point where from one
bit-plane upwards, all the bits will be zero), and (2) the correlation
in the sign bit-plane will be zero. In this situation, an efficient
encoding method is to compress the bit-planes, starting at the MSBP
with at least one bit to 1 (in other words, ``send'' the ones of the
MSBP), sending before the corresponding sign. If more than one one is
found in the MSBP, the bits should be processed by descending
magnitude, i.e., sending first those bits that corresponds with
coefficients with a higher magnitude. This information can be
extracted from the $L$ subband as below is shown.

Information in the $L$ subband can be used to futher reduce the visual
information of $H$ subbands and to estimate the value of its
high-frequency coefficients, and therefore to sort them (at least
approximately) by their magnitude. The idea is to predict the $H$
coefficients by introducing some (visual) information in the $[L]$
subband, and to perform again the 2D-DWT (using the same
wavelets). This generates a $\hat{H}$ subbands that substracted to $H$
should remove the visual information from them (decorrelating the
signs and reducing the number of bits necessary to represent most of
the coefficients).
\begin{equation}
  \tilde{H} = H - \hat{H}
\end{equation}

$\tilde{H}$ is as a prediction error that basically should store
noise. Therefore, if $\tilde{H}$ is not transmitted at all, a good
approximation of $L^0$ should be recontructed. However, in most of
cases, some amount of (unpredictable) visual information will remain
in $\tilde{H}$, concentrated in the MSBPs. The current representation
of the rest of BPs (with unpredictable bits) should be
efficient. Sumarizing, $\tilde{H}$ is a triple subbands of spatially
uncorrelated coefficients, most of them small, with zeros in the MSBPs
and random bits in the LSBPs, and to progressively encode them, a BP
compressor can be used.

At this point, any BLIC (Bi-Level Image Compressor) should be
applicable. However, there is a dependencies between the BPs that
using a BLIC that cannot be exploited. The question is, could it be
possible to predict the coordinates of the ones in a BP? (by default
the BPs are zero). If this is possible, a sequence of bits (with so
many bits as coefficients are in $H$) can encode the success of such
prediction. Suppose that $\hat{H}\downarrow$ (the list of indexes the
sorts $\hat{H}$ in descending order by amplitude) determines the
possible localization of the most energetic coefficients in
$\tilde{H}$. If so, the MSB of the first elements of
$\tilde{H}[\hat{H}\downarrow]$ (the list of residue coefficients
(approximately-) sorted by energy) will be 1 (with a high probability)
and the rest will be 0 (with a high probability). For example, if
there are five 1's in the MSB of the beginning of
$\tilde{H}[\hat{H}\downarrow]$ (that is, a perfect prediction in which
we can localize the position of the most energetic coefficients, but
not its magnitude that is, if their MSB is 1 or 0), the complete BP
can be encoded with the symbols <5><0> (five 1's localized in the
first five positions of $\tilde{H}[\hat{H}\downarrow]$). If for
example, the MSB of the first elements of
$\tilde{H}[\hat{H}\downarrow]$ are 0101001000110... (obviously this
prediction is not perfect), then the output symbols are
<0><1><1><1><2><1><3><2><0>, which can be re-encoded as
<0><1,3><2,1><1,1><3,1><2,1>,<0>.

The output of the last stage is a sequence of symbols with a
exponiental probability distribution that can be encoded efficiently
with a variable-length code. Or using DPCM?

The perfect prediction of the 2nd MSBP of
$\tilde{H}[\hat{H}\downarrow]$ if formed by the first refinements bits
of those coeffientes that already are significative, and by the first
1 of those coefficients that start to being significative in this 2nd
MSBP. Such perfect prediction has the structure
1.(5).10.(7).01.(10).10...0. We can realize now that the first two
runs has a length equal to the number of coefficients significative in
the previous BP, so, we only need to encode the first run of 1's,
generating the symbols <5><0><10><0>. Let's suppose an unperfect
prediction like 1101100|0111100110101100001010...., and that the
number of significant coefficients is 7. In this case, the output is
<2><1><2><0><0><4><2><2><1><1><1><2><4><1><1><1><1><0>.

In the extreme case where the prediction is completely wrong, we are
going to generate a secuence of $N$ bits 0101010101... which can be
encoded as <0><1><1>...<1> ($N$ symbols), which can be re-encoded as
<0><1,N-1>.

%By definition, unpredictable information can not be
%compressed. Consecuently, the lossless encoding of $H$ is basically:
%(1) determine the

The coefficients of $\hat{H}$ sorted by their magnitude in descending
order can be used to send first those bits that belong to the largest
coefficients of $H$.

Obviously, the prediction
\begin{equation}
  \text{sort}(H)==\text{sort}(\hat{H})
\end{equation}
will not always be perfect. This can have two different consequences: (1) that 

Introduced such ideas, to compress the $H$ subbands of a 1-level
2D-DWT, the following algorithm can be used:

\begin{enumerate}

\item Compute $[L]$, zooming-in the $L$ subband.
  \begin{equation}
    [L] = \text{iDWT}(L, 0)
  \end{equation}
  
\item Add to $[L]$ some visual information $V$ that could be present
  in original $L^0$. Any edge enhancement algorithm should work.
  \begin{equation}
    [L'] = [L] + V
  \end{equation}

\item Compute the 2D-DWT of $[L']$ to obtain a prediction $\hat{H}$.
  \begin{equation}
    \_, \hat{H} = \text{DWT}([L'])
  \end{equation}
  Notice that $\_\approx L$.

\item Substract the prediction to the $H$ subbands.
  \begin{equation}
    \tilde{H} = H-\hat{H}
  \end{equation}

\item Sort $\hat{H}$ by magnitude in descending order. For most
  images, $\hat{H}\approx H$ and therefore, the most significant
  coefficients in both structures should be placed in the same
  coordinates.
  \begin{equation}
    \hat{H}\downarrow = \text{unravel\_index}(\text{argsort}(\text{abs}(\hat{H})))
  \end{equation}

\item Go over $\hat{H}\downarrow$, sending by bit-planes the
  sign-magnitude representation of the coefficients of $H$.

  \begin{enumerate}

  \item Estimate the nearest power of two, smaller or equal than the
    coefficient of $H$ with the maximum amplitude.
    \begin{equation}
      \lambda_0 = \lambda = 1 << \text{int}(\log_2(\text{abs}(H_{\hat{H}\downarrow[0]}))).
    \end{equation}
    $\lambda$ should be the index of the MSBP in $H$.
    
  \item Send the BP of index $\lambda$.
    \begin{equation}
      \begin{array}{l}
        \text{for~}H_{s,x,y}\text{~in} H[\hat{H}\downarrow]: \\
        ~ if \lambda > int(\log_2(abs(H_{s,x,y}))) > \lambda/2: \\
        ~~ send(sign(H_{s,x,y}))\\
        ~ 
        %~ 
        %\text{for~}d\text{~in range}(3): \\
        %~ \text{for~}y\text{~in range}(L^0.\text{height}): \\
        %~~ \text{for~}x\text{~in range}(L^0.\text{width}): \\
        %~~~ i = 3\times(\text{height}\times \text{width})+y\times \text{width} + x \\
        %~~~ \text{send}(H[H'[\text{indice}[i]] \text{~mod~} 4][H'[\text{indice}[i]] >> 2]).
      \end{array}
    \end{equation}
  \end{enumerate}
\end{enumerate}
  
\begin{comment}
To compress the $a.H$ subband in the last iteration of the butterfly,
the following algorithm can be used:
\begin{enumerate}
\item [1.] Compute the 2D-DWT of the $a.L$ subband
\begin{equation}
  LL^2, LH^2, HL^2, HH^2 = \text{DWT}(LL). 
\end{equation}

\item [2.] Desplace 2 bits to the left the $H$ coefficients to create
  space for encoding the subband
  \begin{equation}
    \begin{array}{l}
      LH^2 <<= 2 \\
      HL^2 <<= 2 \\
      HH^2 <<= 2.
    \end{array}
  \end{equation}
\item [3.] Label the coefficients of the subbands:
  \begin{equation}
    \begin{array}{l}
      HL^2 += 1 \\
      HH^2 += 2.
    \end{array}
  \end{equation}
\item [4.] Create an array with the 3 matrices
  \begin{equation}
    H = [LH^2, HL^2, HH^2].
  \end{equation}
\item [5.] Create a linear array with the 3 flattened matrices
  \begin{equation}
    H'=\text{ravel}(H).
  \end{equation}
\item [6.] Find the indices for the $LH^2$, $HL^2$ and $HH^2$ matrices
  that sort $H'$ in descending order by energy
  ($\text{width}=LH^2.\text{shape}[0]$ and $\text{height}=LH^2.\text{shape}[1]$)
  \begin{equation}
    \text{indices} =
    \text{unravel\_index}(\text{argsort}(\text{abs}(H')), \text{width},
    \text{height}).
  \end{equation}
\item [7.] Go over $H'$ sending the wavelet coefficients of subbands
  $LH^2$, $HL^2$ and $HH^2$ in descending order by energy
  \begin{equation}
    \begin{array}{l}
      \text{for~}d\text{~in range}(3): \\
      ~ \text{for~}y\text{~in range}(\text{height}): \\
      ~~ \text{for~}x\text{~in range}(\text{width}): \\
      ~~~ i = 3\times(\text{height}\times \text{width})+y\times \text{width} + x \\
      ~~~ \text{send}(H[H'[\text{indice}[i]] \text{~mod~} 4][H'[\text{indice}[i]] >> 2]).
    \end{array}
  \end{equation}
\end{enumerate}
\end{comment}

%}}}
%\end{comment}

\begin{comment}
%{{{

\section{Progressive reconstruction}
The forward bufferfly should reduce the energy of subband $b.H$$ after
substracting a prediction (see Eq.~\ref{eq:weighted_prediction}) which
is generated using the subbands $a.H$ and $c.H$ as references. If the
prediction is good, the energy of $\tilde{b.H}$ will be small and
viceversa. If the prediction is good, the quantization of the subbands
$a.H$, $b.H$ and $c.H$ should leave more energy in the subbands $a.H$
and $c.H$ than in $b.H$. Therefore, if the prediction is good, the
progressive reconstruction of the subbands (using a progressively
small quantization step) should reconstruct first the information of
$a.H$ and $c.H$.

\section{Coefficients encoding}
Subband $\tilde{b.H}$ is not used any more as a reference and
therefore, it can be compressed. The compression algorithm sort the
coefficients of $\tilde{b.H}$ by the amplitude of the coefficients of
$\tilde{b.L}$, considering that the prediction error will affect in
the same way the low and the high frequencies of $b$. Then, the sorted
coefficients are DPCM encoded, quantized, and entropy encoded.

\section{Multilevel MCLT}
The distance between images $a$, $b$ and $c$ should decrease the
efficiency of the predictions, generating more energy in the
low-frequency subbands. Therefore, the quantization when $T>1$ should
transmit more information of the low-frequency subbands than of the
high-frequency ones when the subbands are quantized.

%}}}
\end{comment}
