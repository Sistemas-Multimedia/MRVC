% Emacs, this is -*-latex-*-

% DWT - Discrete Wavelet Transform (critically-sampled octave-band decomposition (R=1 (basis expansion))
% LPT - Laplacian Pyramid Transform (expanded octave-band decomposition (R=2 (frame expansion))
% CS-LPT - Critically Sampled LPT (critically-sampled octave-band decomposition (R=1 (basis expansion))
% ---- CS-SI-DWT - Critically-Sampled Shift-Invariant DWT (spatial critically-sampled octave-band decomposition (R=1 (basis expansion))
% MC-DWT - Motion Compensated DWT (spatial/temporal critically-sampled octave-band decomposition (R=1 (basis expansion))
% MC-CS-LPT - Motion Compensated Critically Sampled LPT (spatoo-temporal critically-sampled octave-band decomposition (R=1 (basis expansion))

%\title{\href{https://github.com/Sistemas-Multimedia/PRMC}{Progressive Resolution Motion Compensation (PRMC)}}
\title{\href{https://github.com/Sistemas-Multimedia/PRMC}{Multi-Resolution Video Coding (MRVC)}}

\author{Vicente Gonz√°lez Ruiz}

%\makeglossaries

\maketitle

\tableofcontents

\section*{Abstract}
%{{{

MRVCs is a transform that inputs a video (a sequence of frames of
pixels) and outputs a decomposition (a sequence of spatio-temporal
subbands of coefficients). It consists of two stages:

\begin{enumerate}
\item MDWT (Motion Discrete Wavelet Transform), that computes the
  1-levels 2D-DWT to every video frame, resulting a representation
  with $2$ SRLs (Spatial Resolution Levels).
\item MCOLP (Motion Compensated in the Orthogonal Laplacian Pyramid),
  which estimates the motion in the lower SRL and compensates the
  motion in the high-frequencies of the higher SRL, using a Orthogonal
  Lapacial Pyramid structure that has a critical representation in the
  2D-DWT domain.
\end{enumerate}

MRVC can be applied recursively to the lower SLR, producing a
multiresolution representation of the video and compensating the
motion of the sequence. The output decomposition concentrates most of
the energy in a small number of subbands (usually, those representing
the low frquencies) which enables the lossy compression of the video.

%}}}

\section{Transform coding and compression}
%{{{

\href{https://en.wikipedia.org/wiki/Video}{Video} data contain high
amounts of \mylink{redundancy}{redundancy}, spatial and temporal. For
this reason, most of \mylink{video_compression}{video encoders}
compress an input sequence of
\href{https://en.wikipedia.org/wiki/Digital_image}{images}\footnote{Also
  called frames, which from a pure mathematical point of view are
  matrices of \href{https://en.wikipedia.org/wiki/Pixel}{pixels}.},
basically, in two stages: (1) a transform in the
\href{https://en.wikipedia.org/wiki/Time_domain}{time} and in the
\href{https://www.quora.com/What-is-spatial-domain-in-image-processing}{spatial}
domains that produces a collection of
\href{https://en.wikipedia.org/wiki/Decorrelation}{uncorrelated}
\href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{subbands},
and (2) an
\href{https://en.wikipedia.org/wiki/Entropy_encoding}{entropy
  encoding} stage which removes the statistical redundancy that can
still remains after the decorrelation (transform). Usually, these
coefficients have two interesting features:
\begin{enumerate}
\item Usually, a \textbf{
  \href{https://vicente-gonzalez-ruiz.github.io/symbol_compression/}{smaller
  entropy}} than the original pixels. This helps to increase
  the \href{https://en.wikipedia.org/wiki/Data_compression_ratio}{compression
    ratio}.
\item A
  \textbf{\href{https://en.wikipedia.org/wiki/Image_resolution}{multiresolution
      representation} (in space and time)} of the visual
  information. This provides the posibility of decoding the sequence
  of images using a smaller resolution. This feature is known as
  \mylink{video_compression}{spatio-temporal scalability in video}.
\item Usually, \textbf{most of the
  \href{https://en.wikipedia.org/wiki/Energy_(signal_processing)}{signal
    energy}} (and therefore, generally, most of the information
  interesting for the humans) \textbf{is represented by
    \href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding}{a
      small number of subbands}}. This helps to improve the
  compression ratio and to prioritize the data in
  \mylink{media_encoding_models}{progressive transmission scenarios}.
\end{enumerate}

%}}}

\section{The 2D-DWT (Distrete Wavelet Transform)}
%{{{

\href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding/#x1-3100020}{The
  2D-DWT (2-Dimensions Distrete Wavelet Transform)\footnote{In this
    document, we will use the term DWT to refeer to the 2D-DWT.}} is a
digital transform that, applied to an image, performs a accumulation
of the energy of the image in a small number of
\href{https://en.wikipedia.org/wiki/Sub-band_coding}{subbands} and
obtains a
\href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding/index.html#x1-3500024}{multiresolution
  (generally dyadic) representation} of such image. The subbands
conform a decomposition\footnote{The structure that forms the subbands
  is called also a decomposition.}  $\{LL, LH, HL, HH\}$, were $L$
stands for 1D low-pass
\href{https://en.wikipedia.org/wiki/Filter_bank}{analysis}
\href{https://en.wikipedia.org/wiki/Filter_(signal_processing)}{filtering}
and $H$ for 1D high-pass analysis filtering. Thus, for example, $LH$
is the result of applying the $L$ filter to the rows and
downsample\footnote{This operation is also called ``decimation'' and
  ``subsampling''.} the output by a factor of 2, and then applying the
$H$ filter to the columns of the previous decomposition and again,
donwsampling by a factor of 2. The subbands are populated by
coefficients representing different spatial areas, depending on the
localization of the coefficient and the resolution level. Notice that
it is possible to compute the (2D-)DWT using 1D filters. Therefore,
the DWT is
\href{http://home.ustc.edu.cn/~xuedixiu/image.ustc/course/dip/DIP14-ch3.pdf}{separable}.

The downsampling operation generates a critically sampled structure,
where the number of output DWT coefficients is equal to the number of
input pixels. This is possible because ideally, the $L$ filter is
designed to reject the frecuencies that the $H$ filter pass, and
viceversa (it is said that the filters are complementary, or that they
form a
\href{https://en.wikipedia.org/wiki/Filter_bank#Perfect_reconstruction_filter_banks}{perfect-reconstruction
  filter bank}). In the practice, there is always some aliasing
(overlaping between the response of the filters in the frequency
domain), but this does not affect\footnote{This affects to the
  idependency of the coefficients in terms of energy contribution to
  the reconstruction of the signal.} to the number of output
coefficients. In terms of
\href{https://en.wikipedia.org/wiki/Linear_algebra}{Linear Algebra},
analysis $L$ and $H$ filters, and the synthesis $L^{-1}$ and $H^{-1}$
filters (the filters that reconstruct the pixels from the coefficients
generated by $L$ and $H$) would be
\href{https://en.wikipedia.org/wiki/Orthogonality}{orthogonal}, that
is, the basis functions that they convolve with the signals are
completely indenpendent.

For the sake of simplicity, we will denote the 1-iteration DWT spatial
decomposition as $\{L, H\}$ where $L=LL$ and $H=\{LH, HL, HH\}$. See
also
\href{https://nbviewer.jupyter.org/github/Sistemas-Multimedia/MRVC/blob/master/docs/DWT_vs_LPT.ipynb}{DWT
  vs LPT}.

%}}}

\subsection{Implementation of the 3 components (usually color) DWT step}
%{{{

\href{https://github.com/Sistemas-Multimedia/MRVC/blob/master/src/DWT.py}{DWT.py}
(see Fig.~\ref{fig:DWT}) implements the forward (DWT) and backward
(iDWT) 1-iteration (step) DWT for color images.

\begin{figure}
  \lstinputlisting[firstline=25, lastline=124, language=python, caption=1-iteration DWT
    (\href{https://github.com/Sistemas-Multimedia/MRVC/blob/master/src/DWT.py}{DWT.py}).]{../src/DWT.py}
  \caption{Implementation of the multicomponent 1-iteration DWT
    (forward and backward). More information about the implementation
    can be found at
    \href{https://pywavelets.readthedocs.io/en/latest/index.html}{PyWavelets}.}
  \label{fig:DWT}
\end{figure}

%}}}

\section{Multi-level DWT structures}
%{{{

A $N$-iterations\footnote{Notice that
  \href{https://pywavelets.readthedocs.io/en/latest/index.html}{PyWavelets}
  uses the term ``levels'' for refering to the number of iterations of
  the 1-level DWT.} dyadic DWT is the result of a recursive use of the
1-iteration DWT applied to the $L$ subband, $N$ times, producing a
$(N+1)$-levels dyadic structure which is able to provide $N+1$ Spatial
Resolution Levels (SRL's). For example, in the
Fig.~\ref{fig:2-levels_DWT} the DWT has been applied 2-times,
generating 3 SRLs: (1) $L^2$, (2) $L^1=\mathtt{iDWT}(L^2, H^2)$, and
(3) $L^0=\mathtt{iDWT}(L^1, H^1)$, where $\mathtt{iDWT}$ stands for
inverse DWT.

\begin{figure}
\begin{verbatim}
    +------+------+-------------+ 0         +------+------+-------------+
    |      |      |             |           |      |      |             |
    | LL^2 | LH^2 |             |           | L^2  |      |             |
    |      |      |             |           |      |      |             |
    +------+------+     LH^1    |           +------+      |             |
    |      |      |             |           |             |             |
    | HL^2 | HH^2 |             |           |        H^2  |             |
    |      |      |             | Y/2-1     |             |             |
    +------+------+-------------+       =   +-------------+             |
    |             |             |           |                           |
    |             |             |           |                           |
    |             |             |           |                           |
    |     HL^1    |     HH^1    |           |                    H^1    |
    |             |             |           |                           |
    |             |             |           |                           |
    |             |             | Y-1       |                           |
    +-------------+-------------+           +---------------------------+
    0           X/2-1         X-1
\end{verbatim}
\caption{Decomposition generated by $\mathtt{DWT}(\mathtt{iters}=2)$ using the standard notation (left) and the compact notation (right).}
\label{fig:2-levels_DWT}
\end{figure}

%}}}

\section{Scalability provided by the DWT}
%{{{

Depending on the number of subband levels (pyramid levels in the
\mylink{pyramids-and-wavelets}{OLP}), we can reconstruct an image at
different spatial scales (resolutions). For example, if a image has
been transformed using the DWT of $2$ iterations (see
Fig.~\ref{fig:2-levels_DWT}), we get:
\begin{itemize}
\item Subband $L^2$ with the scale $2$ of the image. If such image is
  $I$, we will denote the scale $2$ as $I.L^2$.
\begin{verbatim}
    +------+
    |      |
Y/4 |  L^2 |
    |      |
    +------+
       X/4
\end{verbatim}
\item Subband $L^1\coloneqq L=\mathtt{iDWT}(L^2, H^2)$
  with $I.L^1$.
\begin{verbatim}
    +-------------+
    |             |
    |             |
Y/2 |     L^1     |
    |             | 
    |             |
    +-------------+
          X/2
\end{verbatim}
  
\item $L^0=\mathtt{iDWT}(L, H)$ (the original image) with
  the scale $I.L^0=I$.
\begin{verbatim}
    +---------------------------+
    |                           |
    |                           |
    |                           | 
    |                           |
    |                           |
    |                           |
  Y |            L^0            |
    |                           | 
    |                           |
    |                           | 
    |                           | 
    |                           |
    |                           |
    +---------------------------+
                  X
\end{verbatim}
\end{itemize}

%}}}

%{{{ Examples

\subsection{Example: A 1-iteration DWT}
%{{{

An image is transformed, generating four subbands $LL$, $LH$, $HL$ and
$HH$.
\begin{verbatim}
# You must be in the 'src' directory.

# Copy an image (the output directory must be the same as the input one).
rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp

# 1-iteration 2D DWT.
python3 -O DWT.py -p /tmp/ -i 000

# Visualize the subbands.
display -normalize /tmp/LL000.png # <- LL^1
display -normalize /tmp/LH000.png # <- LH^1
display -normalize /tmp/HL000.png # <- HL^1
display -normalize /tmp/HH000.png # <- HH^1

# 1-iteration 2D iDWT.
python3 -O DWT.py -b -p /tmp/ -i 000

# Visualize the reconstruction.
python3 ../tools/substract_offset.py -i /tmp/000.png -o /tmp/1.png
display /tmp/1.png

# Show diferences (PyWavelets uses floating point arithmetic).
../tools/show_differences.sh -1 /tmp/000.png -2 ../sequences/stockholm/000.png -o /tmp/diffs.png
display /tmp/diffs.png
\end{verbatim}

%}}}

\subsection{Example: A 2-iterations DWT ($2\times\mathtt{DWT}$)}
%{{{

\begin{verbatim}
# You must be in the 'src' directory.

# Copy a image (the output directory must be the same than the input one).
rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp

# First iteration.
python3 -O DWT.py -p /tmp/ -i 000

# Second iteration.
python3 -O DWT.py -p /tmp/LL -i 000

# Visualize the subbands.
ls -l /tmp/*.png
display -normalize /tmp/LL000.png # <- LL^1
display -normalize /tmp/LH000.png # <- LH^1
display -normalize /tmp/HL000.png # <- HL^1
display -normalize /tmp/HH000.png # <- HH^1
display -normalize /tmp/LLLL000.png # <- LL^2
display -normalize /tmp/LLLH000.png # <- LH^2
display -normalize /tmp/LLHL000.png # <- HL^2
display -normalize /tmp/LLHH000.png # <- HH^2

# Reverse second iteration.
python3 -O DWT.py -b -p /tmp/LL -i 000
display -normalize /tmp/LL000.png

# Reverse first iteration.
python3 -O DWT.py -b -p /tmp/ -i 000
python3 ../tools/substract_offset.py -i /tmp/000.png -o /tmp/1.png
display /tmp/1.png

# Show diferences.
../tools/show_differences.sh -1 /tmp/000.png -2 ../sequences/stockholm/000.png -o /tmp/diffs.png
display /tmp/diffs.png
\end{verbatim}

%}}}

%}}}

\section{Motion DWT (MDWT)}
%{{{

The 2D-DWT can be applied to a sequence of frames (images) by simply
transforming each one independently. This is done, for example, in the
\mylink{JPEG2000}{Motion JPEG2000 video compression standard}. Notice
that only the spatial redundancy is exploited in MDWT (all the
temporal redundancy still remains in the video). The decomposition
structure generated by MDWT is shown in the
Fig~\ref{fig:MDWT-subbands}. The 1-iteration MDWT inputs a sequence of
frames $\{F_i\}$ and outputs a sequence of decompositions
$\{\{F_i\}.L, \{F_i\}.H\}$, also called sequence spatial subbands. In
all this discussion, $i$ is an integer number starting a $0$ ($F_0$ is
always the first frame of the video). Notice that $\{F_i\}.L$
represents the scale $1$ of the sequence (a version of the original
video $\{F_i\}$ but with a resolution $Y/2\times X/2$), and that
$\{F_i\}.L^0 = \{F_i\} = \mathtt{iMDWT}(\{F_i\}.L,\{F_i\}.H )$. See
the Fig.~\ref{fig:MDWT-implementation} for more details.

\begin{figure}
  \centering \myfig{graphics/forward_MDWT}{6cm}{600}
  \caption{Subbands produced by the MDWT.} %
  \label{fig:MDWT-subbands}
\end{figure}

\begin{figure}
  \lstinputlisting[firstline=15, lastline=65, language=python,
    caption=MDWT
    (\href{https://github.com/Sistemas-Multimedia/MRVC/blob/master/src/MDWT.py}{MDWT.py}).]{../src/MDWT.py}.
  \caption{Implementation of the MDWT.} %
  \label{fig:MDWT-implementation}
\end{figure}

%}}}

\section{Scalability provided by MDWT}
%{{{

MDWT sequences (of (subband) decompositions) are scalable in space and
in time. Spatial scalability is a direct consequence of the DWT
(the number of SRLs can be controlled by the number of iterations of
DWT). On the other hand, it is trivial to observe that MDWT
provides \emph{fully} temporal scalability (we can access to the
images randomly) because each image of the input sequence is
transformed independently.

%}}}

%{{{ Examples

\subsection{Example: 1-iteration MDWT ($\mathtt{MDWT}(N=5)$)}
%{{{

\begin{verbatim}
# You must be in the 'src' directory.
rm /tmp/*.png

# Create the output directory for the 1st-level decompositions.
yes | cp ../sequences/stockholm/*.png /tmp

# 2D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/ -N 5

# Visualize the subbands.
for i in /tmp/LL00?.png; do convert -normalize $i $i.norm; done
animate /tmp/LL*.norm
for i in /tmp/LH00?.png; do convert -normalize $i $i.norm; done
animate /tmp/LH*.norm
for i in /tmp/HL00?.png; do convert -normalize $i $i.norm; done
animate /tmp/HL*.norm
for i in /tmp/HH00?.png; do convert -normalize $i $i.norm; done
animate /tmp/HH*.norm

# 1-iteration iMDWT.
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png
\end{verbatim}

%}}}

\subsection{Example: 2-iterations MDWT ($2\times\mathtt{MDWT}(N=5)$)}
%{{{

\begin{verbatim}
# You must be in the 'src' directory.
rm /tmp/*.png

# Create the output directory for the 1st-level decompositions.
yes | cp ../sequences/stockholm/*.png /tmp

# 2-iterations MDWT.
python3 -O MDWT.py -p /tmp/ -N 5
python3 -O MDWT.py -p /tmp/LL -N 5

# 2-iterations iMDWT.
python3 -O MDWT.py -b -p /tmp/LL -N 5
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png
\end{verbatim}

%}}}

\subsection{Example: Interpolating (a sequence) with the DWT}
%{{{

If we apply an inverse transform to the original frames\footnote{In
  video coding, images are commonly refered as frames.}, supposing
that they are the low-frequency subbands of the input decomposition,
and that the high-frequency subbands are zero, we can interpolate each
frame of the original sequence by a factor of $2$ in each direction.

\begin{verbatim}
# You must be in the 'src' directory.
rm /tmp/*.png

# Copy the current images as LL subbands.
rm /tmp/*.png
for i in {0..4}; do
  image_number=$(printf "%03d" $i)
  cp ../sequences/stockholm/$image_number.png /tmp/LL$image_number.png
done

# Apply the inverse DWT.
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Notice that the dynamic range of the sequence could be changed (the
# images could appear brighter of darker). This is consequence of the
# relative gain of the LL subband in respect of the rest of subbands.
rm /tmp/*.norm
for i in /tmp/???.png; do convert -normalize $i $i.norm; done
animate /tmp/*.png.norm
\end{verbatim}

%}}}

%}}}

\section{Video transform alternatives}
%{{{

To remove both, spatial and temporal redundancies, two different
alternatives are available:
\begin{enumerate}
\item In a \textbf{``t+2D'' transform}, the video is first analyzed
  (transformed) along the time domain and next, over the space domain.
\item A \textbf{``2D+t'' transform} does just the opposite.
\end{enumerate}
Each choice has a number of \emph{pros} and \emph{cons}. For example,
in a ``t+2D'' transform we can apply directly any
\mylink{video_compression}{frame predictor} based on
\href{https://en.wikipedia.org/wiki/Motion_estimation}{Motion
  Estimation (ME)} because the input is a normal video. On the
contrary, if we implement a ``2D+t'' transform, the input to the
motion estimator is a sequence of decompositions.
\href{http://www.polyvalens.com/blog/wavelets/theory}{The overwhelming
  majority of DWT's} are not
\href{http://www.polyvalens.com/blog/wavelets/theory}{shift
  invariant}, which basically means that, exactly the same object
placed in two different frames in different positions will generate
different wavelet coefficients, even if the displacement is an integer
number of pixels and therefore, the pixels of the object in both
frames are identical.  Therefore, motion estimators that compare
coefficient values will not work with accuracy on the decomposition
domain. On the other hand, if we want to provide true spatial
scalability (processing only those
\href{https://www.tutorialspoint.com/dip/spatial_resolution.htm}{spatial
  resolutions} (scales) necessary to get a spatially scaled of our
video), a ``t+2D'' transformed video presents several drawbacks:

\begin{enumerate}
\item The perfect reconstruction of the original frames only is
  possible if the ``t'' stage is identical at both, the
  \href{https://en.wikipedia.org/wiki/Encoder}{encoder} and the
  \href{https://en.wikipedia.org/wiki/Decoder}{decoder}. Therefore, if
  the encoder applies ``t'' at the full resolution (scale 0), the
  decoder must apply ``t'' also at full resolution, even if the
  resolution of the reconstructed frames were smaller. This could be
  unfeasible for receivers with low computational resourcers.
\item Perfect reconstruction can be sacrified by using a quantized
  version of the motion information, but the rate/distortion (R/D)
  tradeoff will be worst. In this case, also, the decoder would be
  wasting motion information.
\item This last problem (the waste of motion information) could be
  avoided if the motion data were encoded in a progressive
  representation (suitable for working in a different spatial
  resolution of the frames). Unfortunately, progressive
  representations usually need more data than non-progressive
  ones. Moreover, it is not trivial to pack the motion data and the
  texture data in a single code-stream to provide all the posible
  types of scalability (temporal, spatial, and quality).
\end{enumerate}

%}}}

\section{DWT structures and Laplacian Pyramids}
%{{{

This, as it will be seen after, shift variance of the wavelet
coefficients is a problem when we want to perform comparisons in the
DWT domain. As it can be supposded, this problem can be mitigated if
the downsampling operation is removed, as happens in the
\href{https://ieeexplore.ieee.org/document/1408191}{Overcomplete DWT}
and in the
\href{http://fourier.eng.hmc.edu/e161/lectures/canny/node3.html}{Laplacian
  Pyramids} (LP). Fortunately, there is an equivalence between DWT
subbands and a family of Laplacian Pyramids that we will call
Orthogonal LP's (OLP's), and it is quite straightforward to
\href{https://vicente-gonzalez-ruiz.github.io/pyramids-and-wavelets/}{convert
  a decomposition representation into a pyramid representation and
  viceversa}, when the $L$ and $H$ filters are used for both, the DWT
and the LP, are orghogonal. If the filters used for generating the LP
were not orthogonal, then it would not be possible to travel from the
LP domain through the DWT domain without lossing information.

%}}}

\section{Motion Compensated Orthogonal Laplacian Pyramid (MCOLP)}
%{{{

MCOLP is a ``2D+t'' transform. The ``2D'' stage is an 1-iteration
MDWT, and the ``t'' stage is basicallt a $T$-iterations 1D
\href{https://en.wikipedia.org/wiki/Motion_compensation}{Motion
  Compensated (MC)} DWT. The MDWT removes the spatial redundancy of
the video, and the temporal DWT removes the temporal redundancy
between $H$ subbands. The number of iterations $T$ determines the
\href{https://en.wikipedia.org/wiki/Group_of_pictures}{GOP} size
\begin{equation}
  G=2^T.
  \label{eq:GOP_size}
\end{equation}

We can think of MCLOPT such as a
\href{https://ieeexplore.ieee.org/abstract/document/1369697}{MCTF}
applied to a given spatial resolution level, and because the motion is
estimated using the previous lower resolution level (which is
available at the decoder), it is not necessary to send the motion
information in the code-stream. Notice also that this process can be
used for any number of spatial resolution levels, simply by using
MCLOPT iteratively on the low spatial resolution level (resulting of
the previous iteration of MRVC that applies first a MDWT).

A peculiar feature of MCLOP is that the predict step used in the
``t''-DWT is performed in the OLP domain, which is shift invariant
because the coefficients have not been downsampled (as happens in the
DWT domain).

%}}}

\subsection{The MCOLP butterfly}
%{{{

The butterly inputs three decompositions $a=\{a.L, a.H\}$, $b=\{b.L,
b.H\}$ and $c=\{c.L, c.H\}$, and outputs a residue subband
$\tilde{b}.H$, which replaces to $b.H$ in the original $b$
decomposition. Therefore, after the use of the bufferfly, we get $a$ ,
$\tilde{b}=\{b.L, \tilde{b}.H\}$ (notice that only the high-frequency
information of $b$ has been compensated) and $c$. This replacement is
fully reversible because the forward transform uses only the
information that the inverse transform will have access to.

Notice that the
\href{http://www.vtvt.ece.vt.edu/research/references/video/DCT_Video_Compression/Zhang92a.pdf}{pyramid
  domain} (which is near-invariant to the pixels displacements) has
been used to estimate and compensate the $H$ subbands. This means that
the motion has been estimated using integer (not sub-) pixel accuracy,
and that the motion has been compensated also integer pixel accuracy
in the OLP domain, that implies $1/2$-pixel accuracy in the DWT
domain.

After the MC, a DWT is computed to $\tilde{b}.H$ in order to get again
a critically sampled version these subbands. After this, the $b.L$
subband is discarded (in fact, replaced by the original subband). The
MC operation can generate information in this subband, because the MC
can produce low-frequency information that could not be represented by
the $H$ subband. However, hopefully the energy of the $L$ subband is
going to be small in most of the cases, and in any case, this is not
an unsolvable problem because the decoder is going to replicate
exactly this behaviour.

\begin{figure}
  \centering \myfig{graphics/forward_butterfly}{10cm}{1000}
  \lstinputlisting[firstline=20, lastline=81, language=python,
    caption={MCOLP butterfly
      (\href{https://github.com/Sistemas-Multimedia/MRVC/blob/master/src/MCOLP.py}{MCOLP.py}).}]{../src/MCOLP.py}
  \lstinputlisting[firstline=4, lastline=9, language=python,
    caption={Computation of the prediction
      frame (\href{https://github.com/Sistemas-Multimedia/MRVC/blob/master/src/simple_average.py}{simple\_average.py}).}]{../src/simple_average.py}
  \caption{Forward and inverse MCOLP butterflies, and computation of
    the prediction frame.}
  \label{fig:forward_butterfly}
\end{figure}

%Notice that the ME projections for $[b.L]$ are performed with $[a.L]$
%and $[c.L]$ instead of $a$ and $c$ (that are available at the decoder)
%because, in $a$ and $c$ there is high-frequency information that can
%dificults the computation of the projections $[a.L]\rightarrow [b.L]$
%and $[c.L]\rightarrow [b.L]$.

%\subsection{The MCOLP step}

%\begin{figure}
%  \centering
%  \svg{forward_MCOLP_step}{1200}
%  \caption{MCOLP forward step.}
%  \label{fig:forward_MCOLP_step}
%\end{figure}
%\subsection{Forward and backward (inverse) transform}

%}}}

\subsection{The MCOLP transform}
%{{{

The $T$-iterations MCOLP (see Fig.~\ref{fig:MCOLP}) is the result of
applying the MCOLP butterfly to all the frames at $T$ different
temporal subsamplings (or scales). At the iteration $1\leq t\leq T$ of
MCOLP, the involved frames are multiple of $2^t$.

\begin{figure}
  \centering
  %\myfig{graphics/temporal_decorrelation_2x1}{10cm}{1000}
  \lstinputlisting[firstline=83, lastline=184, language=python, caption={MCOLP (\href{https://github.com/Sistemas-Multimedia/MRVC/blob/master/src/MCOLP.py}{MCOLP.py}).}]{../src/MCOLP.py}
  \caption{Implementation of the MCOLP.}
  \label{fig:MCOLP}
\end{figure}

Notice that, if the parameter $N=G+1$ (where $G$ is the GOP size, see
Eq.~\ref{eq:GOP_size}), except for the first and the last frame of the
sequence, all the $H$-subbands are compensated.

%}}}

\subsection{Example: $\mathtt{MRVC}(T=1, N=5)$}
%{{{

In this example (see Fig.~\ref{fig:1xMRVC1}), a sequence of five
frames is transformed first with $\mathtt{MDWT}(N=5)$, producing 2
SRLs by frame. Next, the $H$ subbands of the odd frames (frames 1 and
3) are motion compensated using $\mathtt{MCOLP}(T=1, N=5)$.

\begin{verbatim}
predictor=1
iterations=1

# You must be in the 'src' directory to run this.
rm -f /tmp/*.png
cp ../sequences/stockholm/*.png /tmp

# 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/

# Show the length of the subbands.
for i in /tmp/LL???.png; do ls -l $i; done
for i in /tmp/LH???.png; do ls -l $i; done
for i in /tmp/HL???.png; do ls -l $i; done
for i in /tmp/HH???.png; do ls -l $i; done

# 1-iteration MCOLP.
python3 -O MCOLP.py -P $predictor -p /tmp/ -T $iterations

# Show the length of the subbands.
for i in /tmp/LL???.png; do ls -l $i; done
for i in /tmp/LH???.png; do ls -l $i; done
for i in /tmp/HL???.png; do ls -l $i; done
for i in /tmp/HH???.png; do ls -l $i; done

# Has changed in length any of them? Remember that a change in lenght
# implies a change in content.

# Lets recover the original sequence ...
rm /tmp/???.png

# 1-iteration iMCOLP.
python3 -O MCOLP.py -P $predictor -b -p /tmp/ -T $iterations

# 1-iteration iMDWT.
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png
\end{verbatim}

\begin{figure}
  \centering
  \myfig{graphics/temporal_decorrelation_1x1}{10cm}{1000}
  \caption{Subbands generated after running $\mathtt{MRVC}(T=1, N=5)$:
    (1) subband $\{F_i\}.L$, that provides spatial resolution level
    $1$ of the sequence, (2) subband $\{F_{2i}\}.H$, which provides
    spatial resolution level $0$ for the even frames (temporal
    resolution level $1$ at full spatial resolution), and (3) subband
    $\{{\tilde F}_{2i+1}\}.H$, that provides spatial resolution level
    $0$ for the odd frames (temporal resolution level $0$ at full
    spatial resolution, considering that $\{F_{2i}\}.H$ has been
    already decoded).}
  \label{fig:1xMRVC1}
\end{figure}

%}}}

\subsection{Example: $\mathtt{MRVC}(T=2, N=5)$}
%{{{

This example is similar to the previous, but now the number of
iterations of MCOLP is $T=2$, and therefore, the second iteration of
MCOLP compensates also the $H$ subband of the frame 2. This example
matches exactly with the described in the
Fig.~\ref{fig:1xMRVC2}. Notice that the GOP size is $4$ ($3$ TRLs) and
the number of SRLs is $2$.

\begin{verbatim}
predictor=1

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/*.png /tmp

# 1D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/

# Save a copy of the MDWT subbands (for comparing later).
rm -rf /tmp/tmp
mkdir /tmp/tmp
cp /tmp/*.png /tmp/tmp

# 1-iteration MCOLP.
python3 -O MCOLP.py -p /tmp/ -T 1

# Is the content of the MCOLP subbands different to the MDWT's output?
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/LL$ii.png /tmp/tmp/LL$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/LH$ii.png /tmp/tmp/LH$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/HL$ii.png /tmp/tmp/HL$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/HH$ii.png /tmp/tmp/HH$ii.png;echo; done

rm /tmp/*.png
cp ../sequences/stockholm/*.png /tmp

# 1D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/

# Save a copy of the MDWT subbands (for comparing later).
rm -rf /tmp/tmp
mkdir /tmp/tmp
cp /tmp/*.png /tmp/tmp

# 2-iterations MCOLP.
python3 -O MCOLP.py -p /tmp/ -T 2

# Is the content of the MCOLP subbands different to the MDWT's output?
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/LL$ii.png /tmp/tmp/LL$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/LH$ii.png /tmp/tmp/LH$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/HL$ii.png /tmp/tmp/HL$ii.png;echo; done
for i in {0..4}; do ii=$(printf "%03d" $i); ls -l /tmp/HH$ii.png /tmp/tmp/HH$ii.png;echo; done

# Comparing LH visually.
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/tmp/LH$ii.png /tmp/tmp/MDWT_LH$ii.png; done
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/LH$ii.png /tmp/MCOLP_LH$ii.png; done
animate /tmp/tmp/MDWT_LH00?.png &
animate /tmp/MCOLP_LH00?.png &

# Comparing LH002 numerically.
echo MDWT > /tmp/1
printf "%61s\n" MCOLP > /tmp/2
python3 ../tools/show_statistics.py -i /tmp/tmp/LH002.png >> /tmp/1
python3 ../tools/show_statistics.py -i /tmp/LH002.png >> /tmp/2
paste /tmp/1 /tmp/2

# Comparing HL visually.
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/tmp/HL$ii.png /tmp/tmp/MDWT_HL$ii.png; done
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/HL$ii.png /tmp/MCOLP_HL$ii.png; done
animate /tmp/tmp/MDWT_HL00?.png &
animate /tmp/MCOLP_HL00?.png &

# Comparing HL002 numerically.
echo MDWT > /tmp/1
printf "%61s\n" MCOLP > /tmp/2
python3 ../tools/show_statistics.py -i /tmp/tmp/HL002.png >> /tmp/1
python3 ../tools/show_statistics.py -i /tmp/HL002.png >> /tmp/2
paste /tmp/1 /tmp/2

# Comparing HH visually.
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/tmp/HH$ii.png /tmp/tmp/MDWT_HH$ii.png; done
for i in {0..4}; do ii=$(printf "%03d" $i); convert -normalize /tmp/HH$ii.png /tmp/MCOLP_HH$ii.png; done
animate /tmp/tmp/MDWT_HH00?.png &
animate /tmp/MCOLP_HH00?.png &

# Comparing HH002 numerically.
echo MDWT > /tmp/1
printf "%61s\n" MCOLP > /tmp/2
python3 ../tools/show_statistics.py -i /tmp/tmp/HH002.png >> /tmp/1
python3 ../tools/show_statistics.py -i /tmp/HH002.png >> /tmp/2
paste /tmp/1 /tmp/2

# Now we recover the original video.

# 2-iterations iMCOLP.
python3 -O MCOLP.py -P $predictor -b -p /tmp/ -T $iterations

# 1-iteration iMDWT
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png

python3 ../tools/show_statistics.py -i /tmp/diff_002.png
\end{verbatim}

\begin{figure}
  \centering
  \myfig{graphics/temporal_decorrelation_1x2}{10cm}{1000}
  \caption{Subbands generated after running $\mathtt{MRVC}(T=2, N=5)$:
    (1) subband $\{F_i\}.L$, that provides spatial resolution level
    $1$ of the sequence, (2) $\{F_{2^2i}\}.H$, which provides spatial
    resolution level $0$ for frames with number $2^2i$ (temporal
    resolution level $2$ at full spatial resolution), (3) subband
    $\{{\tilde F}_{2^2i+2}\}.H$, that provides spatial resolution
    level $0$ for the frames with number $2^2i+2$ (that considering
    also $\{F_{2^2i}\}.H$, provides temporal resolution level $1$ at
    full spatial resolution), and finally (4) subband $\{{\tilde
      F}_{2i+1}\}.H$, which provides spatial resolution level $0$ for
    odd images (temporal resolution level $0$ at full spatial
    resolution, considering the previouly decoded temporal resolution
    levels).}
  \label{fig:1xMRVC2}
\end{figure}

%}}}

\subsection{Simplification of the notation}
%{{{

For the sake of simplifying the notation, we will represent
$\{F_t\}.L^s$ by $L_t^s$, and $\{F_t\}.H^s$ by $H_t^s$. Thus, for
example, the subbands shown in the Fig.~\ref{fig:1xMRVC2} can be
represented as: (1) $L$ (that is a version of the sequence with
spatial resolution $X/2\times Y/2$), (2) $H_{2^2i}$ (made up of
intra-coded high-frequency spatial information of the frames with
numbers $2^2i$), (3) $\tilde{H}_{2^2i+2}$ (with the high-frequency
spatio-temporal prediction error resulting from decorrelating even
frames with distance $2^2$), and (4) $\tilde{H}_{2i+1}$
(high-frequency spatio-temporal prediction error resulting from
decorrelating frames with distance $2$).

%}}}

\subsection{Example: $\mathtt{MRVC}(T=4, N=17)$}
%{{{

An example with $G=2^4=16$, i.e. $5$ TRLs and $2$ SRLs. The subbands
generated are:
\begin{enumerate}
\item $L$, that by itself, allows to reconstruct all the frames at
  half resolution.
\item $H_{16i+8}$, which with $L$, provides full resolution for frames
  with number $16i+8$ (the rest of frames have half resolution).
\item $H_{8i+4}$, that with the previous subbands provides full
  resolution also for frames with number $8i+4$.
\item $H_{4i+2}$, which also provides full resolution for frames $4i+2$.
\item $H_{2i+1}$, that reconstruct the original sequence.
\end{enumerate}

The temporal decorrelation pattern followed by $\mathtt{iMCOLP(T=4,
  N=17)}$ is (notice that the hexadecimal notation has been followed
for indexing the frames) shown in the Fig.~\ref{fig:MRVC_4_17}.

\begin{figure}
\begin{verbatim}
GOP0                     GOP1
---- ------------------------------------------------
  F0                      F8                      F10 -> H_{8i }  (= TRL3)
              F4                      Fc              -> H_{8i+4} (+ TRL3 = TRL2)
        F2          F6          Fa          Fe        -> H_{4i+2} (+ TRL2 = TRL1)
     F1    F3    F5    F7    F9    Fb    Fd    Ff     -> H_{2i+1} (+ TRL1 = TRL0)
\end{verbatim}
\caption{Motion compensation pattern followed by $\mathtt{MCOLP(T=4,
    N=17)}$. The temporal resolution levels (TRLs) are considering
  only the frames that has been reconstructed at the original spatial
  resolution.}
\label{fig:MRVC_4_17}
\end{figure}

\begin{verbatim}
# You must be in the 'src' directory.
rm /tmp/*.png
video_file=/tmp/akiyo_cif.y4m
if test -f $video_file; then
  print akiyo exists
else
  wget https://media.xiph.org/video/derf/y4m/akiyo_cif.y4m -O /tmp/akiyo_cif.y4m
fi
mplayer /tmp/akiyo_cif.y4m
ffmpeg -i /tmp/akiyo_cif.y4m -vframes 17 -start_number 0 /tmp/%03d.png

# 1D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/ -N 17

# Save a copy of the MDWT subbands (for comparing later).
rm -rf /tmp/tmp
mkdir /tmp/tmp
cp /tmp/LL*.png /tmp/tmp
cp /tmp/LH*.png /tmp/tmp
cp /tmp/HL*.png /tmp/tmp
cp /tmp/HH*.png /tmp/tmp

# 4-iterations MCOLP.
python3 -O MCOLP.py -p /tmp/ -T 4 -N 17

ls -l /tmp/LH*.png > /tmp/1
ls -l /tmp/tmp/LH*.png > /tmp/2
paste /tmp/1 /tmp/2

ls -l /tmp/HL*.png > /tmp/1
ls -l /tmp/tmp/HL*.png > /tmp/2
paste /tmp/1 /tmp/2

ls -l /tmp/HH*.png > /tmp/1
ls -l /tmp/tmp/HH*.png > /tmp/2
paste /tmp/1 /tmp/2

# Now we recover the original video.

# 4-iterations iMCOLP.
python3 -O MCOLP.py -b -p /tmp/ -T 4 -N 17

# 1-iteration iMDWT
python3 -O MDWT.py -b -p /tmp/ -N 17

# Visualization of the reconstruction.
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done
animate /tmp/???.png.png

# Visualization of the residue.
for i in {0..16}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png

python3 ../tools/show_statistics.py -i /tmp/diff_002.png
\end{verbatim}

%}}}

\subsection{Scalability provided by MCOLP}
%{{{

MCOLP preserves the dyadic spatial decomposition generated by MDWT,
and therefore, MCOLP provides the same spatial scalability than MDWT.

Unfortunately, as the rest of video encoders based on MC, MCOLP
reduces the temporal scalability provided by MDWT, allowing (in the
case of MCOLP) only a dyadic access to the frames.

%}}}

\subsection{Example: 3-iters of $\mathtt{MRVC}(T=2, N=5)$}
%{{{

\begin{verbatim}
predictor=1
iterations=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py  -p /tmp/
python3 -O MCOLP.py -p /tmp/     -P $predictor -T $iterations
python3 -O MDWT.py  -p /tmp/LL
python3 -O MCOLP.py -p /tmp/LL   -P $predictor -T $iterations
python3 -O MDWT.py  -p /tmp/LLLL
python3 -O MCOLP.py -p /tmp/LLLL -P $predictor -T $iterations

rm /tmp/00?.png

python3 -O MCOLP.py -p /tmp/LLLL -P $predictor -b -T $iterations
python3 -O MDWT.py  -p /tmp/LLLL               -b
python3 -O MCOLP.py -p /tmp/LL   -P $predictor -b -T $iterations
python3 -O MDWT.py  -p /tmp/LL                 -b
python3 -O MCOLP.py -p /tmp/     -P $predictor -b -T $iterations
python3 -O MDWT.py  -p /tmp/                   -b

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png
\end{verbatim}

In this case, $G=4$, generating $3$ TRLs (see
Fig.~\ref{fig:MRVC_2_5_T}), and $4$ is the number of SRLs (see
Fig.~\ref{fig:MRVC_2_5_S}).

\begin{figure}
\begin{verbatim}

TRLs:

GOP0     GOP1
---- ------------
  F0           F4  -> H_{4i} (= TRL2)
         F2        -> H_{4i+2} (+ TRL2 = TRL1)
      F1    F3     -> H_{2i+1} (+ TRL1 = TRL0)
\end{verbatim}
  \caption{Motion compensation pattern followed by $\mathtt{MCOLP(T=2,
      N=5)}$. The TRLs consider only the frames that has been
    reconstructed at the original spatial resolution.}
  \label{fig:MRVC_2_5_T}
\end{figure}

\begin{figure}
\begin{verbatim}

+---+---+-------+---------------+
| 3 | 2 |   1   |       0       |
+---+   |       |               |
|       |       |               |
+-------+       |               |
|               |               |
|               |               |
|               |               |
+---------------+               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
|                               |
+-------------------------------+
\end{verbatim}
\caption{SRLs generated by $3$ iterations of $\mathtt{MRVC}$. $0$
    corresponds with the original resolution of the frames.}
  \label{fig:MRVC_2_5_S}
\end{figure}

%}}}

\section{Adaptive motion compensation based on the energy of the estimated prediction error}
%{{{

As can be seen in the MCOLP bufferfly (see
Fig.~\ref{fig:forward_butterfly}), both predictions $[b_a.H]$ and
$[b_c.H]$ weight the same in the prediction
\begin{equation}
  [\hat{b.H}] = \frac{[b_a.H] + [b_c.H]}{2}.
\end{equation}
This simple computation, that can have very effective for example in
\href{https://biteable.com/blog/tips/video-transitions-effects-examples/}{dissolves}
video transitions, can also be counterproductive when objects appear
only in one of the reference frames. For this reason, in this section
an improved predictor is proposed, based on the estimated prediction
error (see $[\tilde{b}.H]$ in the Fig.~\ref{fig:forward_butterfly})
generated throughout the ME/MC. To estimate this error, we will
suppose that the prediction error generated for the high frequencies
of an frame is correlated with the prediction error generated for the
low frequencies of such frame.

Let $[b_a.L]$ the prediction generated for the subband $[b.L]$ using
$[a.L]$ as reference and $[a.L]\rightarrow [b.L]$ as motion, and let
$[b_c.L]$ the prediction generated for $[b.L]$ using $[c.L]$ as
reference and $[c.L]\rightarrow [b.L]$ as motion. We now compute the
prediction errors
\begin{equation}
  \begin{array}{l}
    {[e_a.L]} = [b.L] - [b_a.L]\\
    {[e_c.L]} = [b.L] - [b_c.L].
  \end{array}
\end{equation}

We define \emph{similarity matrices} as
\begin{equation}
  \begin{array}{l}
    {[s_a.L]} = \frac{1}{1+{|[e_a.L]|}}\\
    {[s_c.L]} = \frac{1}{1+{|[e_c.L]|}}.    
  \end{array}
  \label{eq:weighted_prediction}
\end{equation}
Notice that, if (for example) the error $[e_a.L]_{x,y}=0$, the
similarity is $[s_a.L]_{x,y}=1$ (the maximum similarity). On the other
side, if the error is high, the similarity tends to be low (but never
$0$).

With this information, that can be recovered by the decoder, the
improved prediction is defined as
\begin{equation}
  [\hat{b.H}] = \frac{[b_a.H][s_a.L]+[s_c.L][b_c.H]}{[s_a.L]+[s_c.L]}.
\end{equation}
The Fig.~\ref{fig:weighted_average} shows an implementation of this
equation.

Notice that, if (for example) $[s_a.L]_{x,y}=1$ and
$[s_c.L]_{x,y}\approx 0$, then
$[\hat{b.H}]_{x,y} \approx [b_a.H]_{x,y}$, and viceversa. If
$[s_a.L]_{x,y}=[s_c.L]_{x,y}$, then
$[\hat{b.H}]_{x,y}=\frac{[b_a.H]_{x,y}+[b_c.H]_{x,y}}{2}$ (even if both similarities are small).

\begin{figure}
  \centering \lstinputlisting[firstline=4, lastline=15,
    language=python, caption={\href{https://github.com/Sistemas-Multimedia/MCOLP/blob/master/src/weighted_average.py}{weighted\_average.py}.}]{../src/weighted_average.py}
  \caption{Computation of the weighted average prediction.}
  \label{fig:weighted_average}
\end{figure}

%}}}

\section{Lossy compression: quantization}
%{{{

\mylink{quantization}{Quantization} allows to remove information from
signals. We will use it in MCOLP to achieve higher compression ratios,
but at the expense of losing visual information (ideally, the less
relevant information for the \mylink{human_visual_system}{HVS} should
be removed first).

When the signal cannot be recoverd perfectly, we can use a
\mylink{distortion_metrics}{distortion metric} to find how much error
has been generated.

%}}}

\subsection{Example: Effects caused by the total attenuation of the motion compensated (MCed) $H$ subbands}
%{{{

\begin{verbatim}
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/*.png /tmp


# 1D 1-iteration MDWT.
python3 -O MDWT.py -p /tmp/

# 1D 1-iteration MCOLP.
python3 -O MCOLP.py -p /tmp/

# Let's delete the motion compensated subbands and reconstruct the sequence ...
rm -f /tmp/LH001.png
rm -f /tmp/HL001.png
rm -f /tmp/HH001.png
rm -f /tmp/LH002.png
rm -f /tmp/HL002.png
rm -f /tmp/HH002.png
rm -f /tmp/LH003.png
rm -f /tmp/HL003.png
rm -f /tmp/HH003.png

# 1D 1-iteration iMCOLP.
python3 -O MCOLP.py -P $predictor -b -p /tmp/

# 1D 1-iteration iMDWT.
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction.
for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

# Visualization of the differences with the original sequence.
rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png

# MSE per frame
for i in {0..4}; do ii=$(printf "%03d" $i); MSE=`python3 -O ../tools/MSE.py -x ../sequences/stockholm/$ii.png -y /tmp/$ii.png`; printf "MSE(%s)=%s\n" $ii $MSE; done

# PSNR per frame
for i in {0..4}; do ii=$(printf "%03d" $i); PSNR=`python3 -O ../tools/PSNR.py -x ../sequences/stockholm/$ii.png -y /tmp/$ii.png`; printf "PSNR(%s)=%s\n" $ii $PSNR; done
\end{verbatim}

%}}}

\subsection{Example: Effects caused by the quantization of MCed $H$ subbands}
%{{{

\begin{verbatim}
q_step=32
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCOLP.py -P $predictor -p /tmp/

#python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/quantize.py -i /tmp/LH001.png -o /tmp/LH001.png -q $q_step
#python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/quantize.py -i /tmp/HL001.png -o /tmp/HL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH001.png -o /tmp/HH001.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH002.png -o /tmp/LH002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL002.png -o /tmp/HL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH002.png -o /tmp/HH002.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH003.png -o /tmp/LH003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL003.png -o /tmp/HL003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH003.png -o /tmp/HH003.png -q $q_step

python3 -O MCOLP.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png

for i in {0..4}; do ii=$(printf "%03d" $i); MSE=`python3 -O ../tools/MSE.py -x ../sequences/stockholm/$ii.png -y /tmp/$ii.png`; printf "MSE(%s)=%s\n" $ii $MSE; done
\end{verbatim}

%}}}

\subsection{Example: Quantization of all $H$ subbands (MCed and intra)}
%{{{

\begin{verbatim}
q_step=32
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCOLP.py -P $predictor -p /tmp/

python3 ../tools/quantize.py -i /tmp/LH000.png -o /tmp/LH000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL000.png -o /tmp/HL000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH000.png -o /tmp/HH000.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH001.png -o /tmp/LH001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL001.png -o /tmp/HL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH001.png -o /tmp/HH001.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH002.png -o /tmp/LH002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL002.png -o /tmp/HL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH002.png -o /tmp/HH002.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH003.png -o /tmp/LH003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL003.png -o /tmp/HL003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH003.png -o /tmp/HH003.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LH004.png -o /tmp/LH004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL004.png -o /tmp/HL004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH004.png -o /tmp/HH004.png -q $q_step

python3 -O MCOLP.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png

for i in {0..4}; do ii=$(printf "%03d" $i); MSE=`python3 -O ../tools/MSE.py -x ../sequences/stockholm/$ii.png -y /tmp/$ii.png`; printf "MSE(%s)=%s\n" $ii $MSE; done
\end{verbatim}

%}}}

\subsection{Example: Quantization of all subbands}
%{{{

\begin{verbatim}
q_step=128
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCOLP.py -P $predictor -p /tmp/

python3 ../tools/quantize.py -i /tmp/LL000.png -o /tmp/LL000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH000.png -o /tmp/LH000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL000.png -o /tmp/HL000.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH000.png -o /tmp/HH000.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LL001.png -o /tmp/LL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH001.png -o /tmp/LH001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL001.png -o /tmp/HL001.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH001.png -o /tmp/HH001.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LL002.png -o /tmp/LL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH002.png -o /tmp/LH002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL002.png -o /tmp/HL002.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH002.png -o /tmp/HH002.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LL003.png -o /tmp/LL003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH003.png -o /tmp/LH003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL003.png -o /tmp/HL003.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH003.png -o /tmp/HH003.png -q $q_step

python3 ../tools/quantize.py -i /tmp/LL004.png -o /tmp/LL004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/LH004.png -o /tmp/LH004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HL004.png -o /tmp/HL004.png -q $q_step
python3 ../tools/quantize.py -i /tmp/HH004.png -o /tmp/HH004.png -q $q_step

python3 -O MCOLP.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b 

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png

for i in {0..4}; do ii=$(printf "%03d" $i); MSE=`python3 -O ../tools/MSE.py -x ../sequences/stockholm/$ii.png -y /tmp/$ii.png`; printf "MSE(%s)=%s\n" $ii $MSE; done
\end{verbatim}

%}}}

\end{document}

%{{{ Temporal interpolation

\begin{comment}
  \subsection{Temporal interpolation}

Hay que trasladar el lifting a este nuevo dise√±o. As√≠ se sabr√° mejor c√≥mo realizar la interpolaci√≥n temporal.
  
  Si conseguimos a->c y c->a, siendo a y c dos im√°genes adyacentes (000 y 001, por ejemplo), podemos generar la imagen predicha proyectando a usando (a->c)/2 y c usando (c->a)/2. As√≠ conseguimos 2 proyecciones, con las que podemos hacer la media o usar el error de predicci√≥n para calcular la proporci√≥n de cada proyecci√≥n.
\begin{verbatim}
predictor=1

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

predictor=1
python3 -O MDWT.py  -p /tmp/
python3 -O MCOLP.py -p /tmp/     -P $predictor -T 1

rm /tmp/00?.png

mv /tmp/LL004.png /tmp/LL008.png
mv /tmp/LL003.png /tmp/LL006.png
mv /tmp/LL002.png /tmp/LL004.png
mv /tmp/LL001.png /tmp/LL002.png
mv /tmp/LH004.png /tmp/LH008.png
mv /tmp/LH003.png /tmp/LH006.png
mv /tmp/LH002.png /tmp/LH004.png
mv /tmp/LH001.png /tmp/LH002.png
mv /tmp/HL004.png /tmp/HL008.png
mv /tmp/HL003.png /tmp/HL006.png
mv /tmp/HL002.png /tmp/HL004.png
mv /tmp/HL001.png /tmp/HL002.png
mv /tmp/HH004.png /tmp/HH008.png
mv /tmp/HH003.png /tmp/HH006.png
mv /tmp/HH002.png /tmp/HH004.png
mv /tmp/HH001.png /tmp/HH002.png

cp /tmp/LL000.png 

python3 -O MCOLP.py -b -p /tmp/ -N 9 -P $predictor -T 2
python3 -O MDWT.py  -b -p /tmp/ -N 9

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png



predictor=1

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

predictor=1
python3 -O MDWT.py  -p /tmp/
python3 -O MCOLP.py -p /tmp/     -P $predictor -T 1

rm /tmp/00?.png

python3 -O MCOLP.py -p /tmp/     -P $predictor -b -T 2
python3 -O MDWT.py  -p /tmp/                   -b

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done
animate /tmp/diff*.png



rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp/LL000.png
cp ../sequences/stockholm/000.png /tmp/LL001.png
cp ../sequences/stockholm/001.png /tmp/LL002.png
cp ../sequences/stockholm/001.png /tmp/LL003.png
cp ../sequences/stockholm/002.png /tmp/LL004.png
cp ../sequences/stockholm/002.png /tmp/LL005.png
cp ../sequences/stockholm/003.png /tmp/LL006.png
cp ../sequences/stockholm/003.png /tmp/LL007.png
cp ../sequences/stockholm/004.png /tmp/LL008.png
cp ../sequences/stockholm/004.png /tmp/LL009.png
python3 -O MCOLP.py -p /tmp/ -N 9 -b
\end{verbatim}        
\end{comment}

%}}}

%{{{ Subpixel accuracy

\begin{comment}
\subsection{Accuracy of the motion compensation (TO BE IMPLEMENTED)}
The movement of the objects in the scene captured by a video is a
continuous event. Conversely, the digital frames taken from such
objects are discrete in the time and the space domains. This means
that, in general, two different positions of an object in two
different frames is not going to match with a distance that can be
measured by an integer number of pixels. For example, lets suppose that the content of the frame $s_0$ is:
\begin{verbatim}
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
  0   0   0 255   0   0   0
\end{verbatim}
and that the camera has moved, following a parallel movement to the
surface of the object of 1/2 pixels. In this case, the frame $s_1$
taken from such object would be:
\begin{verbatim}
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
  0   0 128 128   0   0   0
\end{verbatim}
and it can be seen, it is difficult to determine that the object has
been moved, because the values of the pixels have changed
significantly. However, if we interpolate both frames using the
wavelet ``bior3.5'' of PyWavelets, we obtain for the first
frame\footnote{\texttt{pywt.idwt([0,0,0,255,0,0,0],None,'bior3.5',
    'per')}}:
\begin{verbatim}
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
  0   0   0   0   0  45 135 135  45   0   0   0   0   0
\end{verbatim}
and for the second
one\footnote{\texttt{pywt.idwt([0,0,128,128,0,0,0],None,'bior3.5',
    'per')}}:
\begin{verbatim}
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
  0   0   0  23  68  91  91  68  23   0   0   0   0   0
\end{verbatim}
that is more suitable for determining the sub-pixel accuracy motion.

So, to increase the accuracy of the MC step to 1/2 sub-pixel accuracy
(for example), we can interpolate\footnote{Considering the unkown
  high-frequency information as zero.} all the subbands that are
involved in the MC process by means of a 2-iterations iDWT, generating
$[a.L]^2$, $[c.L]^2$, $[a.H]^2$, $[b.H]^2$ and $[c.H]^2$, and finally,
using a 2-iterations DWT, to obtain $\tilde{b}$.
\end{comment}

%}}}

%\subsection{(Spatial) Multiresolution}
%Spatial dyadic multiresolution can be obtained by appliying a sequence
%of (MDWT+MCOLP) steps (see Fig.~\ref{fig:multiresolution}).

%\begin{figure}
%  \centering %
%  \myfig{graphics/multiresolution}{10cm}{1200}
%  \caption{MDWT+MCOLP multiresolution procedure. The input to the second
%    iteration of the MDWT+MCOLP transform is the output of a previous
%    (first) iteration MDWT+MCOLP transform. The second iteration is only
%    applied to the scale 1.} %
%  \label{fig:multiresolution}
%\end{figure}

%Spatial dyadic multiresolution can be obtained by appliying a sequence
%of MDWT+MCOLP steps (see Fig.~\ref{fig:multiresolution}).


\section{Bit allocation}
En la mariposa (o en un esquema IPPP...) la subbanda $\tilde{H}_b$ compensada depende de las subbandas $L^a^{[q]}$, $L^b$ y $L^c$, para generar los campos de movimiento adecuados. $\tilde{H}_b$ tambi√©n depende de $H^a$ y $H^b$ 

\subsection{Example: Lossy compression of the MC $H$ subbands}
%{{{

\begin{verbatim}
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCOLP.py -P $predictor -p /tmp/

quality=50
python3 ../tools/compress.py -i /tmp/LH001.png -o /tmp/LH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH001.png -o /tmp/LH001.png -f 32640
python3 ../tools/compress.py -i /tmp/HL001.png -o /tmp/HL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL001.png -o /tmp/HL001.png -f 32640
python3 ../tools/compress.py -i /tmp/HH001.png -o /tmp/HH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH001.png -o /tmp/HH001.png -f 32640

python3 ../tools/compress.py -i /tmp/LH002.png -o /tmp/LH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH002.png -o /tmp/LH002.png -f 32640
python3 ../tools/compress.py -i /tmp/HL002.png -o /tmp/HL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL002.png -o /tmp/HL002.png -f 32640
python3 ../tools/compress.py -i /tmp/HH002.png -o /tmp/HH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH002.png -o /tmp/HH002.png -f 32640

python3 ../tools/compress.py -i /tmp/LH003.png -o /tmp/LH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH003.png -o /tmp/LH003.png -f 32640
python3 ../tools/compress.py -i /tmp/HL003.png -o /tmp/HL003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL003.png -o /tmp/HL003.png -f 32640
python3 ../tools/compress.py -i /tmp/HH003.png -o /tmp/HH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH003.png -o /tmp/HH003.png -f 32640

python3 -O MCOLP.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

%}}}

\subsection{Example: Lossy compression of all $H$ subbands}
%{{{

\begin{verbatim}
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCOLP.py -P $predictor -p /tmp/

quality=10
python3 ../tools/compress.py -i /tmp/LH000.png -o /tmp/LH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH000.png -o /tmp/LH000.png -f 32640
python3 ../tools/compress.py -i /tmp/HL000.png -o /tmp/HL000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL000.png -o /tmp/HL000.png -f 32640
python3 ../tools/compress.py -i /tmp/HH000.png -o /tmp/HH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH000.png -o /tmp/HH000.png -f 32640

python3 ../tools/compress.py -i /tmp/LH001.png -o /tmp/LH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH001.png -o /tmp/LH001.png -f 32640
python3 ../tools/compress.py -i /tmp/HL001.png -o /tmp/HL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL001.png -o /tmp/HL001.png -f 32640
python3 ../tools/compress.py -i /tmp/HH001.png -o /tmp/HH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH001.png -o /tmp/HH001.png -f 32640

python3 ../tools/compress.py -i /tmp/LH002.png -o /tmp/LH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH002.png -o /tmp/LH002.png -f 32640
python3 ../tools/compress.py -i /tmp/HL002.png -o /tmp/HL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL002.png -o /tmp/HL002.png -f 32640
python3 ../tools/compress.py -i /tmp/HH002.png -o /tmp/HH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH002.png -o /tmp/HH002.png -f 32640

python3 ../tools/compress.py -i /tmp/LH003.png -o /tmp/LH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH003.png -o /tmp/LH003.png -f 32640
python3 ../tools/compress.py -i /tmp/HL003.png -o /tmp/HL003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL003.png -o /tmp/HL003.png -f 32640
python3 ../tools/compress.py -i /tmp/HH003.png -o /tmp/HH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH003.png -o /tmp/HH003.png -f 32640

python3 ../tools/compress.py -i /tmp/LH004.png -o /tmp/LH004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH004.png -o /tmp/LH004.png -f 32640
python3 ../tools/compress.py -i /tmp/HL004.png -o /tmp/HL004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL004.png -o /tmp/HL004.png -f 32640
python3 ../tools/compress.py -i /tmp/HH004.png -o /tmp/HH004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH004.png -o /tmp/HH004.png -f 32640

python3 -O MCOLP.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

%}}}

\subsection{Example: Lossy compression of all subbands}
%{{{

\begin{verbatim}
predictor=2

# You must be in the 'src' directory.
rm /tmp/*.png
cp ../sequences/stockholm/* /tmp

python3 -O MDWT.py -p /tmp/
python3 -O MCOLP.py -P $predictor -p /tmp/

quality=10
python3 ../tools/compress.py -i /tmp/LL000.png -o /tmp/LL000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL000.png -o /tmp/LL000.png -f 32640
python3 ../tools/compress.py -i /tmp/LH000.png -o /tmp/LH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH000.png -o /tmp/LH000.png -f 32640
python3 ../tools/compress.py -i /tmp/HL000.png -o /tmp/HL000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL000.png -o /tmp/HL000.png -f 32640
python3 ../tools/compress.py -i /tmp/HH000.png -o /tmp/HH000.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH000.png -o /tmp/HH000.png -f 32640

python3 ../tools/compress.py -i /tmp/LL001.png -o /tmp/LL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL001.png -o /tmp/LL001.png -f 32640
python3 ../tools/compress.py -i /tmp/LH001.png -o /tmp/LH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH001.png -o /tmp/LH001.png -f 32640
python3 ../tools/compress.py -i /tmp/HL001.png -o /tmp/HL001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL001.png -o /tmp/HL001.png -f 32640
python3 ../tools/compress.py -i /tmp/HH001.png -o /tmp/HH001.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH001.png -o /tmp/HH001.png -f 32640

python3 ../tools/compress.py -i /tmp/LL002.png -o /tmp/LL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL002.png -o /tmp/LL002.png -f 32640
python3 ../tools/compress.py -i /tmp/LH002.png -o /tmp/LH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH002.png -o /tmp/LH002.png -f 32640
python3 ../tools/compress.py -i /tmp/HL002.png -o /tmp/HL002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL002.png -o /tmp/HL002.png -f 32640
python3 ../tools/compress.py -i /tmp/HH002.png -o /tmp/HH002.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH002.png -o /tmp/HH002.png -f 32640

python3 ../tools/compress.py -i /tmp/LL003.png -o /tmp/LL003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL003.png -o /tmp/LL003.png -f 32640
python3 ../tools/compress.py -i /tmp/LH003.png -o /tmp/LH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH003.png -o /tmp/LH003.png -f 32640
python3 ../tools/compress.py -i /tmp/HL003.png -o /tmp/HL003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL003.png -o /tmp/HL003.png -f 32640
python3 ../tools/compress.py -i /tmp/HH003.png -o /tmp/HH003.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH003.png -o /tmp/HH003.png -f 32640

python3 ../tools/compress.py -i /tmp/LL004.png -o /tmp/LL004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LL004.png -o /tmp/LL004.png -f 32640
python3 ../tools/compress.py -i /tmp/LH004.png -o /tmp/LH004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/LH004.png -o /tmp/LH004.png -f 32640
python3 ../tools/compress.py -i /tmp/HL004.png -o /tmp/HL004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HL004.png -o /tmp/HL004.png -f 32640
python3 ../tools/compress.py -i /tmp/HH004.png -o /tmp/HH004.png -q $quality
python3 ../tools/add_offset.py -i /tmp/HH004.png -o /tmp/HH004.png -f 32640

python3 -O MCOLP.py -P $predictor -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

%}}}

\section{MCOLP decomposition}
\end{comment}

\begin{comment}
  % Libro vetterli

  Perceptual quantization:
  
  An important consideration is the relative perceptual importance of
  the subbands. This liead to a weighting of the MSE in the
  subbands. This weighting function can be derived throudh perfectual
  experiments by finding the level of ``just noticeable noise''. As
  expected, high subbands tolerate more noise because the HVS becomes
  less senitive at high frequencies.
\end{comment}

\begin{comment}
  % Libro vetterli

  Subbands are rarely independent (usually, some kind of prediction
  can be applied between the different scales). However, because the
  subbands are downsampled versions of the original subbands, the
  coefficients are shift-variant. This reduces the effectiviness, for
  example, of VQ applied across the subbands.
\end{comment}

\begin{comment}
\section{MCDWT decomposition}
%{{{

\begin{figure}
  \centering %
  \myfig{graphics/MCOLP_decomposition}{12cm}{1200} %
  \caption{MCOLP($S=1$, $T=3$) decomposition (notice that
    $\text{GOP\_size}=8=2^T$). The arrows indicate frame dependencies
    (for example, to decode frame 1, frames 0 and 2 should have been
    decoded.} %
  \label{fig:MCOLP_decomposition}
\end{figure}

MCOLP($S$, $T$), where $S$ is the number of levels of the spatial
transform and $T$ is the number of levels of the temporal transform,
decomposes a sequence of frames into a sequence of subbands organized
in $T+1$ of temporal scales of $S+1$ spatial scales (see
Fig.~\ref{fig:MCOLP_decomposition}).

%}}}

\section{Encoding of B-type H-subbands (experimental)}
%{{{

The subband $\tilde{b}.H$ generated by the MCOLP butterfly (see
Sec.~\ref{sec:butterfly}) is no longer needed for applying the
butterfly to different decompositions and scales. For this reason, we
can compress this subband. As a result, after using the butterfly to
<<<<<<< all the decompositions of the sequence and all the scales, all the $H$
subbands can be compressed, excepting the intra-coded frame of each
GOP. Notice that the redundancy exploited in B-type $H$ subbands is
temporal.

A $\tilde{b}.H$ subband can be compressed by sorting its coefficients
by energy and predicting them. Thus, the better the prediction the
higher the compression ratio. To sort the coefficients of subband
$\tilde{b}.H$ by energy without using it (remember that the decoder
does not know this information), we can think that there is a
correlation between the coefficients of $\tilde{b}.H$ and
$\tilde{b}.L$, that is, the prediction error resulting of substracting
to $b.L$ a prediction $\hat{b}.L$ generated with the same motion
information used to built the prediction $\hat{b}.H$. Such idea has
been implemented in the following algorithm:

\begin{enumerate}
\item [1.] Compute the prediction error for the $[b.L]$ subband
\begin{equation}
  [\tilde{b}.L] = [b.L] - [\hat{b}.L]
\end{equation}
where (see Eq.~\ref{eq:weighted_prediction})
\begin{equation}
  [\hat{b}.L] = \frac{[b_a.L][s_a.L]+[s_c.L][b_c.L]}{[s_a.L]+[s_c.L]}.
\end{equation}

\item [2.] Compute the 2D-DWT of subband $L$
  \begin{equation}
    \tilde{b}=\text{DWT}([\tilde{b}.L]).
  \end{equation}

\item [3.] Find the indices for $\tilde{b}.L$ that sorts it in descending order by energy
  \begin{equation}
    \text{indices}=\text{unravel\_index}(\text{argsort}(\text{abs}(\tilde{b}.L),
    b.L.\text{width}, b.L.\text{height})).
  \end{equation}
%\item [3.] For each coefficient $x,y$ in $\tilde{b}.L$:
%  \begin{enumerate}
%  \item [a.] $d_{i=x\times b.\text{width}+y} = (\text{abs}(\tilde{b}_{x,y}), x, y)$
%  \end{enumerate}
%\item [4.] Sort $d$ in desceding order sorted by field 0 /* use the amplitude of luminance */.
\item Go over $\tilde{b}.L$ sending the wavelet coefficients of subbands
  $LH^2$, $HL^2$ and $HH^2$ in descending order by energy.
%\item [5.] Estimate the nearest power of two smaller or equan than the maximum amplitude
%  \begin{equation}
%    \lambda_0 = \lambda = 1 << \text{int}(\log_2(\text{abs}(d_{0}))).
%  \end{equation}
%\item [6.] For each coefficient $i$ in $d$:
%  \begin{enumerate}
%  \item [a.] $x,y = d_i[1:2]$.
%  \item [b.] If any of
%    $1<<\text{int}(\log_2(\tilde{b.H}_{x,y}))==\lambda_0$ then send
%    $\text{sign}(\tilde{b.H}_{x,y})$ for the three (LH, HL and HH)
%    subbands.
%  \item [b.] Send $(\hat{b.H}_{x,y}~\text{bitwise-and}~\lambda)/\lambda$ for
%    the three (LH, HL and HH) subbands.
%  \end{enumerate}
\item [7.] $\lambda >>= 1$.
\item [8.] Goto step 6.
\end{enumerate}

%}}}

\section{Encoding of I-type H-subbands (experimental)}
%{{{

After a $T$-levels MCOLP transform of a sequence, $T+1$ temporal
subbands are generated, and the temporal subband $L^{T+1}$ has a one
I-type subband for each GOP (see
Fig~\ref{fig:MCOLP_decomposition}). Notice that, in a I-type frame,
the redundancy exploited to accumulate the visual information in the
$L$ subband (or to remove the visual information in the $H$ subbands)
is spatial.

After using MCOLP, I-type frames are decomposed into
the four subbands $LL$ (low-frequencies), $LH$, $HL$ and $HH$
(high-frequencies). Depending on the frame content and wavelets used
in the 2D-DWT, $H$ subbands contain a certain amount of \emph{visual
  information}\footnote{Any visual stimulus that provides information
  to humans}. The key here is to reduce as much as possible such
visual information, leaving in the $H$ subbands only \emph{visual
  noise}\footnote{Any visual stimulus that does not provides
  information to humans.} If we achieve this, the $H$ subbands,
expressed in a sign/magnitude representation will have two features:
(1) the probability of finding zeros when we are moving from the least
significant bit-planes fo the most significant ones of the magnitude
of the coefficients will increase (to the point where from one
bit-plane upwards, all the bits will be zero), and (2) the correlation
in the sign bit-plane will be zero. In this situation, an efficient
encoding method is to compress the bit-planes, starting at the MSBP
with at least one bit to 1 (in other words, ``send'' the ones of the
MSBP), sending before the corresponding sign. If more than one one is
found in the MSBP, the bits should be processed by descending
magnitude, i.e., sending first those bits that corresponds with
coefficients with a higher magnitude. This information can be
extracted from the $L$ subband as below is shown.

Information in the $L$ subband can be used to futher reduce the visual
information of $H$ subbands and to estimate the value of its
high-frequency coefficients, and therefore to sort them (at least
approximately) by their magnitude. The idea is to predict the $H$
coefficients by introducing some (visual) information in the $[L]$
subband, and to perform again the 2D-DWT (using the same
wavelets). This generates a $\hat{H}$ subbands that substracted to $H$
should remove the visual information from them (decorrelating the
signs and reducing the number of bits necessary to represent most of
the coefficients).
\begin{equation}
  \tilde{H} = H - \hat{H}
\end{equation}

$\tilde{H}$ is as a prediction error that basically should store
noise. Therefore, if $\tilde{H}$ is not transmitted at all, a good
approximation of $L^0$ should be recontructed. However, in most of
cases, some amount of (unpredictable) visual information will remain
in $\tilde{H}$, concentrated in the MSBPs. The current representation
of the rest of BPs (with unpredictable bits) should be
efficient. Sumarizing, $\tilde{H}$ is a triple subbands of spatially
uncorrelated coefficients, most of them small, with zeros in the MSBPs
and random bits in the LSBPs, and to progressively encode them, a BP
compressor can be used.

At this point, any BLIC (Bi-Level Image Compressor) should be
applicable. However, there is a dependencies between the BPs that
using a BLIC that cannot be exploited. The question is, could it be
possible to predict the coordinates of the ones in a BP? (by default
the BPs are zero). If this is possible, a sequence of bits (with so
many bits as coefficients are in $H$) can encode the success of such
prediction. Suppose that $\hat{H}\downarrow$ (the list of indexes the
sorts $\hat{H}$ in descending order by amplitude) determines the
possible localization of the most energetic coefficients in
$\tilde{H}$. If so, the MSB of the first elements of
$\tilde{H}[\hat{H}\downarrow]$ (the list of residue coefficients
(approximately-) sorted by energy) will be 1 (with a high probability)
and the rest will be 0 (with a high probability). For example, if
there are five 1's in the MSB of the beginning of
$\tilde{H}[\hat{H}\downarrow]$ (that is, a perfect prediction in which
we can localize the position of the most energetic coefficients, but
not its magnitude that is, if their MSB is 1 or 0), the complete BP
can be encoded with the symbols <5><0> (five 1's localized in the
first five positions of $\tilde{H}[\hat{H}\downarrow]$). If for
example, the MSB of the first elements of
$\tilde{H}[\hat{H}\downarrow]$ are 0101001000110... (obviously this
prediction is not perfect), then the output symbols are
<0><1><1><1><2><1><3><2><0>, which can be re-encoded as
<0><1,3><2,1><1,1><3,1><2,1>,<0>.

The output of the last stage is a sequence of symbols with a
exponiental probability distribution that can be encoded efficiently
with a variable-length code. Or using DPCM?

The perfect prediction of the 2nd MSBP of
$\tilde{H}[\hat{H}\downarrow]$ if formed by the first refinements bits
of those coeffientes that already are significative, and by the first
1 of those coefficients that start to being significative in this 2nd
MSBP. Such perfect prediction has the structure
1.(5).10.(7).01.(10).10...0. We can realize now that the first two
runs has a length equal to the number of coefficients significative in
the previous BP, so, we only need to encode the first run of 1's,
generating the symbols <5><0><10><0>. Let's suppose an unperfect
prediction like 1101100|0111100110101100001010...., and that the
number of significant coefficients is 7. In this case, the output is
<2><1><2><0><0><4><2><2><1><1><1><2><4><1><1><1><1><0>.

In the extreme case where the prediction is completely wrong, we are
going to generate a secuence of $N$ bits 0101010101... which can be
encoded as <0><1><1>...<1> ($N$ symbols), which can be re-encoded as
<0><1,N-1>.

%By definition, unpredictable information can not be
%compressed. Consecuently, the lossless encoding of $H$ is basically:
%(1) determine the

The coefficients of $\hat{H}$ sorted by their magnitude in descending
order can be used to send first those bits that belong to the largest
coefficients of $H$.

Obviously, the prediction
\begin{equation}
  \text{sort}(H)==\text{sort}(\hat{H})
\end{equation}
will not always be perfect. This can have two different consequences: (1) that 

Introduced such ideas, to compress the $H$ subbands of a 1-level
2D-DWT, the following algorithm can be used:

\begin{enumerate}

\item Compute $[L]$, zooming-in the $L$ subband.
  \begin{equation}
    [L] = \text{iDWT}(L, 0)
  \end{equation}
  
\item Add to $[L]$ some visual information $V$ that could be present
  in original $L^0$. Any edge enhancement algorithm should work.
  \begin{equation}
    [L'] = [L] + V
  \end{equation}

\item Compute the 2D-DWT of $[L']$ to obtain a prediction $\hat{H}$.
  \begin{equation}
    \_, \hat{H} = \text{DWT}([L'])
  \end{equation}
  Notice that $\_\approx L$.

\item Substract the prediction to the $H$ subbands.
  \begin{equation}
    \tilde{H} = H-\hat{H}
  \end{equation}

\item Sort $\hat{H}$ by magnitude in descending order. For most
  frames, $\hat{H}\approx H$ and therefore, the most significant
  coefficients in both structures should be placed in the same
  coordinates.
  \begin{equation}
    \hat{H}\downarrow = \text{unravel\_index}(\text{argsort}(\text{abs}(\hat{H})))
  \end{equation}

\item Go over $\hat{H}\downarrow$, sending by bit-planes the
  sign-magnitude representation of the coefficients of $H$.

  \begin{enumerate}

  \item Estimate the nearest power of two, smaller or equal than the
    coefficient of $H$ with the maximum amplitude.
    \begin{equation}
      \lambda_0 = \lambda = 1 << \text{int}(\log_2(\text{abs}(H^{\hat{H}\downarrow[0]}))).
    \end{equation}
    $\lambda$ should be the index of the MSBP in $H$.
    
  \item Send the BP of index $\lambda$.
    \begin{equation}
      \begin{array}{l}
        \text{for~}H^{s,x,y}\text{~in} H[\hat{H}\downarrow]: \\
        ~ if \lambda > int(\log_2(abs(H^{s,x,y}))) > \lambda/2: \\
        ~~ send(sign(H^{s,x,y}))\\
        ~ 
        %~ 
        %\text{for~}d\text{~in range}(3): \\
        %~ \text{for~}y\text{~in range}(L^0.\text{height}): \\
        %~~ \text{for~}x\text{~in range}(L^0.\text{width}): \\
        %~~~ i = 3\times(\text{height}\times \text{width})+y\times \text{width} + x \\
        %~~~ \text{send}(H[H'[\text{indice}[i]] \text{~mod~} 4][H'[\text{indice}[i]] >> 2]).
      \end{array}
    \end{equation}
  \end{enumerate}
\end{enumerate}

%}}}
  
%{{{ To compress the $a.H$ subband

To compress the $a.H$ subband in the last iteration of the butterfly,
the following algorithm can be used:
\begin{enumerate}
\item [1.] Compute the 2D-DWT of the $a.L$ subband
\begin{equation}
  LL^2, LH^2, HL^2, HH^2 = \text{DWT}(LL). 
\end{equation}

\item [2.] Desplace 2 bits to the left the $H$ coefficients to create
  space for encoding the subband
  \begin{equation}
    \begin{array}{l}
      LH^2 <<= 2 \\
      HL^2 <<= 2 \\
      HH^2 <<= 2.
    \end{array}
  \end{equation}
\item [3.] Label the coefficients of the subbands:
  \begin{equation}
    \begin{array}{l}
      HL^2 += 1 \\
      HH^2 += 2.
    \end{array}
  \end{equation}
\item [4.] Create an array with the 3 matrices
  \begin{equation}
    H = [LH^2, HL^2, HH^2].
  \end{equation}
\item [5.] Create a linear array with the 3 flattened matrices
  \begin{equation}
    H'=\text{ravel}(H).
  \end{equation}
\item [6.] Find the indices for the $LH^2$, $HL^2$ and $HH^2$ matrices
  that sort $H'$ in descending order by energy
  ($\text{width}=LH^2.\text{shape}[0]$ and $\text{height}=LH^2.\text{shape}[1]$)
  \begin{equation}
    \text{indices} =
    \text{unravel\_index}(\text{argsort}(\text{abs}(H')), \text{width},
    \text{height}).
  \end{equation}
\item [7.] Go over $H'$ sending the wavelet coefficients of subbands
  $LH^2$, $HL^2$ and $HH^2$ in descending order by energy
  \begin{equation}
    \begin{array}{l}
      \text{for~}d\text{~in range}(3): \\
      ~ \text{for~}y\text{~in range}(\text{height}): \\
      ~~ \text{for~}x\text{~in range}(\text{width}): \\
      ~~~ i = 3\times(\text{height}\times \text{width})+y\times \text{width} + x \\
      ~~~ \text{send}(H[H'[\text{indice}[i]] \text{~mod~} 4][H'[\text{indice}[i]] >> 2]).
    \end{array}
  \end{equation}
\end{enumerate}

%}}}

\section{Progressive reconstruction}
%{{{

The forward bufferfly should reduce the energy of subband $b.H$$ after
substracting a prediction (see Eq.~\ref{eq:weighted_prediction}) which
is generated using the subbands $a.H$ and $c.H$ as references. If the
prediction is good, the energy of $\tilde{b.H}$ will be small and
viceversa. If the prediction is good, the quantization of the subbands
$a.H$, $b.H$ and $c.H$ should leave more energy in the subbands $a.H$
and $c.H$ than in $b.H$. Therefore, if the prediction is good, the
progressive reconstruction of the subbands (using a progressively
small quantization step) should reconstruct first the information of
$a.H$ and $c.H$.

%}}}

\section{Coefficients encoding}
%{{{

Subband $\tilde{b.H}$ is not used any more as a reference and
therefore, it can be compressed. The compression algorithm sort the
coefficients of $\tilde{b.H}$ by the amplitude of the coefficients of
$\tilde{b.L}$, considering that the prediction error will affect in
the same way the low and the high frequencies of $b$. Then, the sorted
coefficients are DPCM encoded, quantized, and entropy encoded.

%}}}

\section{Multilevel MCLT}
%{{{

The distance between frames $a$, $b$ and $c$ should decrease the
efficiency of the predictions, generating more energy in the
low-frequency subbands. Therefore, the quantization when $T>1$ should
transmit more information of the low-frequency subbands than of the
high-frequency ones when the subbands are quantized.

%}}}
