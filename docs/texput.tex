\title{Motion Compensated Discrete Wavelet Transform (MCDWT)}
\author{Vicente Gonz√°lez Ruiz}
%\makeglossaries
\maketitle
\tableofcontents

\section{Intro}

\href{https://en.wikipedia.org/wiki/Video}{Video} data contain high
amounts of \mylink{redundancy}{redundancy}, spatial and temporal. For
this reason, most of \mylink{video_compression}{video encoders}
compress an input sequence of
\href{https://en.wikipedia.org/wiki/Digital_image}{images}\footnote{Which
  from a pure mathematical point of view are matrices of
  \href{https://en.wikipedia.org/wiki/Pixel}{pixels}.}, basically, in
two stages: (1) a transform stage in the
\href{https://en.wikipedia.org/wiki/Time_domain}{time} and in the
\href{https://www.quora.com/What-is-spatial-domain-in-image-processing}{spatial}
domains that produces a collection of
\href{https://en.wikipedia.org/wiki/Decorrelation}{uncorrelated}
\href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{coefficients}, and (2) an
\href{https://en.wikipedia.org/wiki/Entropy_encoding}{entropy
  encoding} phase which removes the statistical redundancy that can
still remains after the decorrelation (transform). These coefficients
have two interesting features:
\begin{enumerate}
\item Usually, a \textbf{
  \href{https://vicente-gonzalez-ruiz.github.io/symbol_compression/}{smaller
  entropy}} than the original pixels. This helps to increase
  the \href{https://en.wikipedia.org/wiki/Data_compression_ratio}{compression
    ratio}.
\item A
  \textbf{\href{https://en.wikipedia.org/wiki/Image_resolution}{multiresolution
      representation} (in space and time)} of the visual
  information. This provides the posibility of decoding the sequence
  of images using a smaller resolution (in space and time). This
  feature is known as \mylink{video_compression}{spatial/temporal
    scalability in video}.
\item Usually, \textbf{most of the
  \href{https://en.wikipedia.org/wiki/Energy_(signal_processing)}{energy}}
  (and therefore, generally, most of the information interesting for
  the humans) \textbf{is represented by
    \href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding}{a
      small number of coefficients}}. This helps to improve the
  compression ratio and to prioritize the data in
  \mylink{media_encoding_models}{progressive transmission scenarios}.
\end{enumerate}

\section{The 2D DWT (Distrete Wavelet Transform)}
\href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding/#x1-3100020}{The
  2D\footnote{In this document, we will use the term DWT to refeer to
    the 2D DWT.} DWT (Distrete Wavelet Transform)} is a digital
transform that, applied to an image, performs a spatial decorrelation
and obtains a
\href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding/index.html#x1-3500024}{multiresolution
  (generally dyadic) representation} of such image, conforming a
collection of DWT subbands. The forward transform converts the image
into a set of frequency subbands with coefficients representing
different spatial areas and frequency orientations. There is an
equivalence between DWT subbands and
\href{http://fourier.eng.hmc.edu/e161/lectures/canny/node3.html}{Laplacian
  Pyramids}, and it is quite simple to
\href{https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/pyramids-and-wavelets/index.ipynb}{pass
  from a decomposition\footnote{The structure that forms the subbands
    is called also a decomposition.} representation to a pyramid
  representation and viceversa}.

\subsection{Implementation of the 3 components (color) DWT step}
\href{https://github.com/vicente-gonzalez-ruiz/MCDWT/blob/master/src/DWT.py}{DWT.py}
(see Fig.~\ref{fig:DWT}) implements the forward and backward
1-iteration (step) 2D DWT for color images.

\begin{figure}
\lstinputlisting[firstline=9, lastline=105, language=python, caption=DWT.py]{../src/DWT.py}
\caption{Implementation of the color DWT step. $L$ and $H$ stands for
  \emph{low-pass filtered} and \emph{high-pass filtered},
  respectively.  More information about the implementation can be
  found at
  \href{https://pywavelets.readthedocs.io/en/latest/index.html}{PyWavelets}.}
\label{fig:DWT}
\end{figure}

Notice that the $N$-iterations DWT can be computed y applying the DWT
step $N$ times, recursively, to the subband $LL$.

\subsection{Multi-level DWT structures}
A dyadic 0DWT is a recursive use of the
(1-iteration\footnote{\href{https://pywavelets.readthedocs.io/en/latest/index.html}{PyWavelets}
  uses the term ``levels'' for refering to the number of iterations of
  the 1-interation DWT, and athough levels and interations are closely
  related ($k+1$ spatial resolution levels are provided by the
  $k$-iterations 2D DWT), we will distinguish between iterations and
  spatial resolution levels.}) 2D-DWT applied to the $LL$ subband. For
example, if the 2D DWT is applied $k$-times to the $LL$ subband,
recursively, a decomposition of spatial resolution $k+1$-levels is
generated. For the sake of simplicity, we will denote the subbands
$\{LH^k, HL^k, HH^k\}$ as only $H^k$, and $LL^k$ as only $L^k$. See
\href{https://nbviewer.jupyter.org/github/Sistemas-Multimedia/MCDWT/blob/master/docs/DWT_vs_LPT.ipynb}{DWT
  versus LPT}.

\subsection{Scalability provided by the 2D DWT}
Depending on the number of subband levels (pyramid levels in the
\mylink{https://pywavelets.readthedocs.io/en/latest/index.html}{pyramid
  representation}), we can reconstruct an image at different spatial
scales (resolutions). For example, if an image $s_1$ has been
transformed using the DWT of 2 iterations, we get:
\begin{itemize}
\item Subband $s_1.L^2$ with the scale 2.
\begin{verbatim}
    +------+
    |      |
Y/4 |  L2  |
    |      |
    +------+
       X/4
\end{verbatim}
\item Subband $s_1.L^1=s_1.L=\text{iDWT}(s_1.L^2, S_1.H^2)$
  ($\text{iDWT}=\text{inverse DWT}=\text{backward DWT}$) with the
  scale $1$.
\begin{verbatim}
    +-------------+
    |             |
    |             |
Y/2 |     L1      |
    |             | 
    |             |
    +-------------+
          X/2
\end{verbatim}
  
\item $s_1.L^0=\text{iDWT}(s_1.L, s_1.H)$ (the original image) with
  the scale $0$.
\begin{verbatim}
    +---------------------------+
    |                           |
    |                           |
    |                           | 
    |                           |
    |                           |
    |                           |
  Y |            L0             |
    |                           | 
    |                           |
    |                           | 
    |                           | 
    |                           |
    |                           |
    +---------------------------+
                  X
\end{verbatim}
\end{itemize}

\subsection{A 1-iteration 2D DWT example}
\begin{verbatim}
# You must be in the 'src' directory.

# Copy an image (the output directory must be the same as the input one):
rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp

# 1-iteration 2D forward DWT:
python3 -O DWT.py -p /tmp/ -i 000

# Visualize the subbands:
display -normalize /tmp/LL000.png # <- LL1
display -normalize /tmp/LH000.png # <- LH1
display -normalize /tmp/HL000.png # <- HL1
display -normalize /tmp/HH000.png # <- HH1

# 1-iteration 2D backward DWT:
python3 -O DWT.py -b -p /tmp/ -i 000

# Visualize the reconstruction:
display -normalize /tmp/000.png 

# Show diferences (PyWavelets uses floating point arithmetic):
../tools/show_differences.sh -1 /tmp/000.png -2 ../sequences/stockholm/000.png -o /tmp/diffs.png
display -normalize /tmp/diffs.png
\end{verbatim}

\subsection{A 2-iterations 2D DWT example}
\begin{verbatim}
# You must be in the 'src' directory.

# Copy a image (the output directory must be the same than the input one):
rm /tmp/*.png
cp ../sequences/stockholm/000.png /tmp

# First iteration:
python3 -O DWT.py -p /tmp/ -i 000

# Second iteration:
python3 -O DWT.py -p /tmp/LL -i 000

# Visualize the subbands:
ls -l /tmp/*.png
display -normalize /tmp/LL000.png # <- LL1  
display -normalize /tmp/LH000.png # <- LH1  
display -normalize /tmp/HL000.png # <- HL1  
display -normalize /tmp/HH000.png # <- HH1  
display -normalize /tmp/LLLL000.png # <- LL2
display -normalize /tmp/LLLH000.png # <- LH2
display -normalize /tmp/LLHL000.png # <- HL2
display -normalize /tmp/LLHH000.png # <- HH2

# Reverse second iteration:
python3 -O DWT.py -b -p /tmp/LL -i 000
display -normalize /tmp/LL000.png

# Reverse first iteration:
python3 -O DWT.py -b -p /tmp/ -i 000
display -normalize /tmp/000.png

# Show diferences:
../tools/show_differences.sh -1 /tmp/000.png -2 ../sequences/stockholm/000.png -o /tmp/diffs.png
display -normalize /tmp/diffs.png
\end{verbatim}

\section{Motion DWT (MDWT)}
The DWT can be applied to a sequence of images by simply transforming
each image of the sequence independently. This is done, for example,
in the \mylink{JPEG2000}{Motion JPEG2000 video compression
  standard}. Notice that only the spatial redundancy is exploited in
MDWT. All the temporal redundancy still remains in the video. The
decomposition structure generated by MDWT is shown in the
Fig~\ref{fig:forward_MDWT}. 1-iteration MDWT inputs a sequence of
images $\{s_i\}$ and outputs a sequence of decompositions $\{S_i\}$,
each one providing 2 spatial resolution levels ($\{s_i.L\}$ and
$\{s_i.L^0\}=\{s_i\}$) of the image sequence.

\begin{figure}
  \centering
  \myfig{graphics/forward_MDWT}{6cm}{600}
  \caption{Forward 1-iteration MDWT step.}
  \label{fig:forward_MDWT}
\end{figure}

\lstinputlisting[firstline=16, lastline=66, language=python, caption=MDWT.py]{../src/MDWT.py}

\subsection{Scalability provided by MDWT}
MDWT sequences (of (subband) decompositions) are scalable in space and
in time. Spatial scalability is a direct consequence of the 2D DWT
(the number of spatial resolution levels can be generated bycontrolled by the number
of iterations of DWT). On the other hand, it is trivial to observe
that MDWT provides \emph{fully} temporal scalability (we can access to
the images randomly) because each image of the input sequence is
transformed independently.

\subsection{1-iteration MDWT example}
\begin{verbatim}
# You must be in the 'src' directory.

# Create the output directory for the 1st-level decompositions:
yes | cp ../sequences/stockholm/*.png /tmp

# 2D 1-iteration forward MDWT:
python3 -O MDWT.py -p /tmp/ -N 5

# Visualize the subbands:
for i in /tmp/LL00?.png; do convert -normalize $i $i.norm; done
animate /tmp/LL*.norm
for i in /tmp/LH00?.png; do convert -normalize $i $i.norm; done
animate /tmp/LH*.norm
for i in /tmp/HL00?.png; do convert -normalize $i $i.norm; done
animate /tmp/HL*.norm
for i in /tmp/HH00?.png; do convert -normalize $i $i.norm; done
animate /tmp/HH*.norm

# 2D 1-iteration backward MDWT:
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction:
rm /tmp/*.norm
for i in /tmp/00?.png; do convert -normalize $i $i.norm; done; animate /tmp/00?.png.norm
\end{verbatim}

\subsection{2-iterations MDWT example}
\begin{verbatim}
# You must be in the 'src' directory.

# Create the output directory for the 1st-level decompositions:
yes | cp ../sequences/stockholm/*.png /tmp

# 2D 2-iterations forward MDWT:
python3 -O MDWT.py -p /tmp/ -N 5
python3 -O MDWT.py -p /tmp/LL -N 5


# 2D 2-iterations backward MDWT:
python3 -O MDWT.py -b -p /tmp/LL -N 5
python3 -O MDWT.py -b -p /tmp/ -N 5

# Visualization of the reconstruction:
rm /tmp/*.norm
for i in /tmp/00?.png; do convert -normalize $i $i.norm; done; animate /tmp/00?.png.norm
\end{verbatim}

\section{Video transform alternatives}
To remove both, spatial and temporal redundancies, two different
alternatives are available: (1) t+2D transforms and (2) 2D+t
transforms. In a t+2D transform, the video is first
\href{https://en.wikipedia.org/wiki/Digital_filter\#Analysis_techniques}{analyzed}
(transformed) over the time domain and next, over the space domain. A
2D+t transform does just the opposite. Each choice has a number of
\emph{pros} and \emph{cons}. For example, in a t+2D transform we can
apply directly any image predictor based on
\href{https://en.wikipedia.org/wiki/Motion_estimation}{motion
  estimation} because the input is a normal video. However, if we
implement a 2D+t transform, the input to the motion estimator is a
sequence decompositions.
\href{http://www.polyvalens.com/blog/wavelets/theory}{The overwhelming
  majority of DWT's} are not
\href{http://www.polyvalens.com/blog/wavelets/theory}{shift
  invariant}, which basically means that, exactly the same object
placed in two different images in different positions will generate
different wavelet coefficients, even if the displacement is an integer
number of pixels and therefore, the pixels of the object in both
images are identical.  Therefore, motion estimators which compare
coefficient values will not work with accuracy on the decomposition
domain. On the other hand, if we want to provide true spatial
scalability (processing only those
\href{https://www.tutorialspoint.com/dip/spatial_resolution.htm}{spatial
  resolutions} (scales) necessary to get a spatially scaled of our
video), a t+2D transformed video presents some drawbacks:

\begin{enumerate}
\item The perfect reconstruction of the original images only is
  possible of the t stage is identical at both, the encoder and the
  decoder. Therefore, if the encoder apply t at the full resolution
  (scale 0), the decoder must apply t at full resolution, also, even
  if the resolution of the reconstructed images were smaller. This
  could be unfeasible for receivers with low computational resourcers.
\item Perfect reconstruction can be sacrified using a quantized
  version of the motion information, but the rate/distortion (R/D)
  tradeoff will be worst. In this case, also, the decoder would be
  wasting motion information.
\item A solution for this last problem (the waste of motion
  information) could be avoided if the motion data is encoded in a
  progressive representation (each one working in a different spatial
  resolution of the images). Unfortunately, progressive
  representations use to need more data than non-progressive ones.
\end{enumerate}

Finally, it is important to realize that the presence of the motion
data in the code-stream introduces also complexity into the decoding
process because, in a quality scalable scenario, for example, it is
not trivial (specially when non-linear systems such as those based on
ME are involved) to decide how to interleave the motion and the
texture information in a code-stream that can be decoded partially
depending on the avaliable bandwidth.

% Subband Motion Compensated Temporal Filtering (SMCTF) 
\section{Motion Compensated Discrete Wavelet Transform (MCDWT)}
MCDWT is a 2D+t transform. The 2D stage is MDWT, and it is applied
first. The t stage is a 1D DWT, which removes the temporal redundancy
between adjacent $H$ subbands by means of
\href{https://en.wikipedia.org/wiki/Motion_compensation}{Motion
  Compensation (MC)}. A special characteristic of MCDWT is that the
motion information is not transmitted from the
\href{https://en.wikipedia.org/wiki/Encoder}{encoder} to the
\href{https://en.wikipedia.org/wiki/Decoder}{decoder}, which must
replicate it or at least, predict it. This is possible because to
estimate de motion at the encoder, only the information that it is
accesible by the decoder is used.

\subsection{The MCDWT butterfly}
MCDWT butterly inputs three decompositions $a=\{a.L, a.H\}$, $b=\{b.L,
b.H\}$ and $c=\{c.L, c.H\}$, and outputs tree decompositions $a=\{a.L,
a.H\}$, $\tilde{b}=\{b.L, \tilde{b}.H\}$ and $c=\{c.L, c.H\}$, where
$\tilde{b}.H$ is a residue subband which replaces to $b.H$ in the
original $b$ decomposition. Therefore, after the use of the bufferfly,
we get $a$ (an
\href{https://en.wikipedia.org/wiki/Video_compression_picture_types}{intra-coded
  ``I'' image}), $\tilde{b}$ (a
\href{https://en.wikipedia.org/wiki/Video_compression_picture_types}{bidirectionally
  predicted ``B'' image}) and $c$ (another intra-coded ``I''
image). This replacement is fully reversible because the forward
transform uses only the information that the backward transform
will have access to. Notice that the
\href{http://www.vtvt.ece.vt.edu/research/references/video/DCT_Video_Compression/Zhang92a.pdf}{pyramid
  domain} (which is invariant to the pixels displacements) has been
used to estimate and compensate the $H$ subbands.

\begin{figure}
  \centering
  \myfig{graphics/forward_butterfly}{10cm}{1200}
  \caption{Forward MCDWT butterfly.}
  \label{fig:forward_butterfly}
\end{figure}
\lstinputlisting[firstline=17, lastline=54, language=python, caption={MCDWT.py (extract)}]{../src/MCDWT.py}
\lstinputlisting[firstline=4, lastline=9, language=python, caption={simple\_average.py (extract)}]{../src/simple_average.py}

Notice that, even if $a$ and $c$ (at full resolution) are available at
the decoder, only $b.L$ is. So, it does not make sense to search the
low resolution structures of $b.L$ in the high resolution images $a$
and $c$, because even if a perfect match is achieved (think about the
three decompositions $a$, $b$ and $c$ are identical), the prediction error
would be different than $0$ as a consequence of that the high
resolution information is missing in $b.L$.

\subsection{The MCDWT step}
A step of the MCDWT is carried out when the MCDWT butterfly is applied
to all the images of the input sequece using the pattern shown in the
Fig.~\ref{fig:forward_MCDWT_step}. Notice that only the images with an
odd number are transformed.
\begin{figure}
  \centering
  \svg{forward_MCDWT_step}{1200}
  \caption{MCDWT forward step.}
  \label{fig:forward_MCDWT_step}
\end{figure}

\subsection{The forward MCDWT}
The forward MCDWT is the result of appliying the forward MCDWT step
recursively, to the image with number $2^i$, where $i$ is the
iteration of the MCDWT step. As a result, chooding $i$ larte enough,
except for the first and the last image of the sequence, all the
$H$-subbands are temporally decorrelated.

\begin{figure}
  \centering
  \myfig{graphics/temporal_decorrelation}{10cm}{1200}
  \caption{MCDWT forward transform.}
  \label{fig:forward_MCDWT}
\end{figure}
\lstinputlisting[firstline=69, lastline=138, language=python, caption={MCDWT.py (extract)}]{../src/MCDWT.py}

\subsection{(Spatial) Multiresolution}
Spatial dyadic multiresolution can be obtained by appliying a sequence
of (MDWT+MCDWT) steps (see Fig.~\ref{fig:multiresolution}).

\begin{figure}
  \centering
  \myfig{graphics/multiresolution}{10cm}{1200}
  \caption{MDWT+MCDWT multiresolution procedure. The input to the second
    iteration of the MDWT+MCDWT transform is the output of a previous
    (first) iteration MDWT+MCDWT transform. The second iteration is only
    applied to the scale 1.}
  \label{fig:multiresolution}
\end{figure}

\subsection{Scalability provided by MCDWT}
The spatial decomposition generated by MCDWT is identical to the
produced by MDWT, and therefore, MCDWT provides the same spatial
scalability than MDWT. In order to obtain several spatial resolutions,
MCDWT should be applied to the low-frequency subbands, recursively. As
it is shown in the example of the Figure~\ref{fig:multiresolution},
MCDWT has been applied to the output of the example of the
Figure~\ref{fig:forward_MCDWT}. Notice that between two consecutive
iterations of MCDWT, MDWT must be applied to the low-frequency subband
in order to reduce the resolution of the analysis.

On the other hand, MCDWT decreases the temporal scalability offered by
MDWT to allowing only dyadic access (depending on required image and
the number of MCDWT levels, more than one decomposition needs to be
inversely transformed). For example, if only one step of the backward
transform is applied to the example of the
Figure~\ref{fig:forward_MCDWT}, only the decompositions of $s_0$,
$s_2$ and $s_4$ will be reconstructed (the second
\href{https://en.wikipedia.org/wiki/Temporal_resolution}{temporal
  resolution}). If a second iteration of the backward transform is
used, all the decompositions are rendered.

\subsection{MCDWT 1-levels example}
\begin{verbatim}
# You must be in the 'src' directory.

yes | cp ../sequences/stockholm/*.png /tmp

# 1D 1-levels forward MDWT:
python3 -O MDWT.py -p /tmp/

# 1D 1-levels forward MCDWT:
python3 -O MCDWT.py -p /tmp/

rm /tmp/00?.png

# 1D 1-levels backward MCDWT:
python3 -O MCDWT.py -b -p /tmp/

# 1D 1-levels backward MDWT:
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction:
for i in /tmp/???.png; do python3 ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/???.png.png
\end{verbatim}

\subsection{Showing some statistics}
\begin{verbatim}
# You must be in the 'src' directory.

cp ../sequences/stockholm/*.png /tmp

# 1D 1-levels forward MDWT:
python3 -O MDWT.py -p /tmp/

# First and last decompositions are not transformed by MCDWT (step 1/2):
ls -l /tmp/HH000.png
ls -l /tmp/HL004.png

# The rest, are transformed (step 1/2):
ls -l /tmp/HL001.png
ls -l /tmp/LH002.png
ls -l /tmp/HH003.png

# Showing statistics (step 1/2):
python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/show_statistics.py -i /tmp/HL001.png
python3 ../tools/show_statistics.py -i /tmp/HH001.png
python3 ../tools/show_statistics.py -i /tmp/LH002.png
python3 ../tools/show_statistics.py -i /tmp/HL002.png
python3 ../tools/show_statistics.py -i /tmp/HH002.png

# 1D 1-levels forward MCDWT:
python3 -O MCDWT.py -p /tmp/

# First and last decompositions are not transformed by MCDWT step 2/2):
ls -l /tmp/HH000.png
ls -l /tmp/HL004.png

# The rest, are transformed (step 2/2):
ls -l /tmp/HL001.png
ls -l /tmp/LH002.png
ls -l /tmp/HG003.png

# Showing statistics:
python3 ../tools/show_statistics.py -i /tmp/LH001.png
python3 ../tools/show_statistics.py -i /tmp/HL001.png
python3 ../tools/show_statistics.py -i /tmp/HH001.png
python3 ../tools/show_statistics.py -i /tmp/LH002.png
python3 ../tools/show_statistics.py -i /tmp/HL002.png
python3 ../tools/show_statistics.py -i /tmp/HH002.png

# Visualization of the H-subbands
for i in /tmp/LH00?.png; do convert -normalize $i $i.png; done; animate /tmp/LH00?.png.png
for i in /tmp/HL00?.png; do convert -normalize $i $i.png; done; animate /tmp/HL00?.png.png
for i in /tmp/HH00?.png; do convert -normalize $i $i.png; done; animate /tmp/HH00?.png.png
\end{verbatim}

\subsection{Filtering the MC subbands}
\begin{verbatim}
# You must be in the 'src' directory.

cp ../sequences/stockholm/*.png /tmp

# 1D 1-levels forward MDWT:
python3 -O MDWT.py -p /tmp/

# 1D 1-levels forward MCDWT:
python3 -O MCDWT.py -p /tmp/

# Let's clear the motion compensated subbands and reconstruct the sequence ...

# Create a zero subband:
bash ../tools/create_black_image.sh; python ../tools/add_offset.py -i /tmp/zero.png -o /tmp/zero16.png -f 32768
python3 ../tools/show_statistics.py -i /tmp/zero16.png

# "Delete" the motion compensated subbands:
yes | cp /tmp/zero16.png /tmp/LH001.png
yes | cp /tmp/zero16.png /tmp/HL001.png
yes | cp /tmp/zero16.png /tmp/HH001.png
yes | cp /tmp/zero16.png /tmp/LH002.png
yes | cp /tmp/zero16.png /tmp/HL002.png
yes | cp /tmp/zero16.png /tmp/HH002.png
yes | cp /tmp/zero16.png /tmp/LH003.png
yes | cp /tmp/zero16.png /tmp/HL003.png
yes | cp /tmp/zero16.png /tmp/HH003.png

# 1D 1-levels backward MCDWT:
python3 -O MCDWT.py -b -p /tmp/

# 1D 1-levels backward MDWT:
python3 -O MDWT.py -b -p /tmp/

# Visualization of the reconstruction:
for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png

# Visualization of the differences with the original sequence:
rm -f /tmp/diff*.png; for i in {0..4}; do ii=$(printf "%03d" $i); bash ../tools/show_differences.sh -1 /tmp/$ii.png -2 ../sequences/stockholm/$ii.png -o /tmp/diff_$ii.png; done; animate /tmp/diff*.png
\end{verbatim}

\subsection{2D-MDWT-MCDWT 2-levels example}
\begin{verbatim}
# You must be in the 'src' directory.

yes | cp ../sequences/stockholm/* /tmp
python3 -O MDWT.py -p /tmp/
python3 -O MCDWT.py -p /tmp/
python3 -O MDWT.py -p /tmp/LL
python3 -O MCDWT.py -p /tmp/LL

rm /tmp/00?.png

python3 -O MCDWT.py -p /tmp/LL -b
python3 -O MDWT.py -p /tmp/LL -b
python3 -O MCDWT.py -p /tmp/ -b
python3 -O MDWT.py -p /tmp/ -b

for i in /tmp/00?.png; do python ../tools/substract_offset.py -i $i -o $i.png; done; animate /tmp/00?.png.png
\end{verbatim}

\section{Adaptive motion compensation based on the distortion of the prediction error}
As can be seen in the MCDWT bufferfly (see
Fig.~\ref{fig:forward_butterfly}), both predictions $[b_a.H]$ and
$[b_c.H]$ have the same weight to build the prediction
\begin{equation}
  [\hat{b.H}] = \frac{[b_a.H] + [b_c.H]}{2}.
\end{equation}
This simple computation, that can have very effective for example in
\href{https://biteable.com/blog/tips/video-transitions-effects-examples/}{dissolves}
video transitions, can also be counterproductive when objects appear
only in one of the reference images. For this reason, in this section
a different predictor is proposed, based on the prediction error
generated during the ME process.

Let $[b_a.L]$ the prediction generated for the subband $[b.L]$ using
$[a.L]$ as reference and $[a.L]\rightarrow [b.L]$ as motion, and let
$[b_c.L]$ the prediction generated for $[b.L]$ using $[c.L]$ as
reference and $[c.L]\rightarrow [b.L]$ as motion. We now compute the
prediction errors
\begin{equation}
  \begin{array}{l}
    {[e_a.L]} = [b.L] - [b_a.L]\\
    {[e_c.L]} = [b.L] - [b_c.L].
  \end{array}
\end{equation}

Now, we define the similarity matrices as
\begin{equation}
  \begin{array}{l}
    {[s_a.L]} = \frac{1}{1+{|[e_a.L]|}}\\
    {[s_c.L]} = \frac{1}{1+{|[e_c.L]|}}.    
  \end{array}
  \label{eq:weighted_prediction}
\end{equation}
Notice that, if (for example) the error $[e_a.L]_{x,y}=0$, the
similarity is $[s_a.L]_{x,y}=1$ (the maximum similarity), and if the
error is high, the similarity tends to be $0$.

With this information, that can be recovered by the decoder, the
improved prediction is defined as
\begin{equation}
  [\hat{b.H}] = \frac{[b_a.H][s_a.L]+[s_c.L][b_c.H]}{[s_a.L]+[s_c.L]}.
\end{equation}

Notice that, if (for example) $[s_a.L]_{x,y}=1$ and
$[s_c.L]_{x,y}\approx 0$, then
$[\hat{b.H}]_{x,y}\approx [b_a.H]_{x,y}$, and viceversa. If
$[s_a.L]_{x,y}=[s_c.L]_{x,y}$, then
$[\hat{b.H}]_{x,y}=\frac{[b_a.H]+[b_c.H]}{2}$ (even if both similarities are small).

\lstinputlisting[firstline=4, lastline=15, language=python, caption={weighted\_average.py (extract)}]{../src/weighted_average.py}

\section{Orthogonal, orthonormal, and biorthogonal transforms}
In signal processing\footnote{In mathemathics the definition of
  orthogonality refers to characteristics such as the basis of the
  transform forms an orthognal espace, where is is impossible to
  represent one of the basis as a linear combination of the rest of
  basis (or in other words, if the inner product is zero).}, a
transform (such as the Discrete Cosine Transform, the Walsh-Hadamard
Transform or the Karhunen-Loeve Transform) is orthogonal when the
coefficients generated by the transform are uncorrelated (there is no
way to infer one coefficient from another).

If the norm of all the basis of the an orthogonal transform is one,
then the transform is said orthonormal. Orthonormal transforms are
interesting because:
\begin{enumerate}
\item They are energy preserving: the energy of the output is the same
  than the energy of the input. This means that, for example, a
  quantization error produced in a coefficient of the transform will
  generate the same quantization error at the output (the complete
  signal) of the inverse transform. The same holds by the forward
  transform.
\item The transform matrix of the inverse transform is the transpose
  of the forward transform. In orthogonal transforms, the transform
  matrix of the inverse transform is the inverse of the transform
  matrix of the forward transform.
\end{enumerate}

Biorthogonal transforms do no satisfy any of these features: they are
not energy preserving (this can be also observed because the
frequency-domain responses of the analysis and synthesis filters are
not symmetric), and there is not an algebraic way (matrix
transposition/inversion) to compute the backward transform from the
forward one, and viceversa. This, that can be considered as a
drawback, gives an extra degree of freedom to design the analysis and
the synthesis filters (whose only requirement is that transform pair
to be reversible), providing in general the possibility of using more
sofisticated filters such as those based on non-linear filtering, as
for example, those based on motion estimation algorithms.

In general, each subband $b$ of a decomposition generated by a
biorthogonal 2D-DWT transform have a different subband gain
$\alpha_b$. Usually, the lower the frequency of the subband, the
higher the gain. Notice also, that these gains also are different for
each different transform.

Subband gains are important in lossy signal compression because they
quantify the relative importance of the wavelet coefficients of the
different subbands when we introduce distortion in the wavelet
domain. Thus, for example, if we decide to quantize a wavelet
coefficient, the amount of distortion that we are generating in the
signal domain will depend on the subband where that coefficient is
localized. In general, low-frequency coefficients are more
``important'' that high-frequency ones.

To compute the subband gains we have two options:
\begin{enumerate}
\item The algebraic way. We will need the expressions of the four
  filters (two anaylsis filters and two synthesis filters) and deduce
  the gains.
\item The algorithmic way. We will need to compute the energy of the
  impulse response of the inverse transform, when we apply such
  impulse to each one of the subbands of the decomposition. Supposing
  that, after appliying the DWT to an image, the coefficients of the
  subband HH are the least energetic with an energy $x$, the subband
  gain for subband $b$ is computed as
  \begin{equation}
    \alpha_b = \epsilon_b/x,
  \end{equation}
  where $\epsilon_b$ is the energy of the reconstruction when the
  impulse signal is localized at $b$. Notice that all the gains should
  be larger than one.
\end{enumerate}

This computation can be also applied to MCDWT, by computing the subband
gains as a function of the number $T$ of temporal decompositions.

\begin{comment}

\section{Encoding of B-type subbands}
The subband $\tilde{b.H}$ is no longer needed for applying the
butterfly to different decompositions and scales. For this reason, we
can compress this subband. As a result, after using the butterfly to
all the decompositions of the sequence and all the scales, all the $H$
subbands can be compressed, excepting the intra-coded image of each
GOP. Notice that the redundancy exploited in B-type $H$ subbands is
temporal.

To compress the $\tilde{b.H}$ subbands, the following algorithm can be
used:
\begin{enumerate}
\item [1.] Compute the prediction error for the $[b.L]$ subband
\begin{equation}
  [\tilde{b.L}] = [b.L] - [\hat{b.L}]
\end{equation}
where
\begin{equation}
  [\hat{b.L}] = \frac{[b_a.L][s_a.L]+[s_c.L][b_c.L]}{[s_a.L]+[s_c.L]}.
\end{equation}

\item [2.] Compute the 2D-DWT of subband $L$
  \begin{equation}
    \tilde{b}=\text{DWT}([\tilde{b.L}]).
  \end{equation}

\item [3.] Find the indices for $\tilde{b.L}$ that sorts it in descending order by energy
  \begin{equation}
    \text{indices}=\text{unravel\_index}(\text{argsort}(\text{abs}(\tilde{b.L}),
    b.L.\text{width}, b.L.\text{height})).
  \end{equation}
%\item [3.] For each coefficient $x,y$ in $\tilde{b}.L$:
%  \begin{enumerate}
%  \item [a.] $d_{i=x\times b.\text{width}+y} = (\text{abs}(\tilde{b}_{x,y}), x, y)$
%  \end{enumerate}
%\item [4.] Sort $d$ in desceding order sorted by field 0 /* use the amplitude of luminance */.
\item Go over $\tilde{b}.L$ sending the wavelet coefficients of subbands
  $LH^2$, $HL^2$ and $HH^2$ in descending order by energy
%\item [5.] Estimate the nearest power of two smaller or equan than the maximum amplitude
%  \begin{equation}
%    \lambda_0 = \lambda = 1 << \text{int}(\log_2(\text{abs}(d_{0}))).
%  \end{equation}
%\item [6.] For each coefficient $i$ in $d$:
%  \begin{enumerate}
%  \item [a.] $x,y = d_i[1:2]$.
%  \item [b.] If any of
%    $1<<\text{int}(\log_2(\tilde{b.H}_{x,y}))==\lambda_0$ then send
%    $\text{sign}(\tilde{b.H}_{x,y})$ for the three (LH, HL and HH)
%    subbands.
%  \item [b.] Send $(\hat{b.H}_{x,y}~\text{bitwise-and}~\lambda)/\lambda$ for
%    the three (LH, HL and HH) subbands.
  \end{enumerate}
\item [7.] $\lambda >>= 1$.
\item [8.] Goto step 6.
\end{enumerate}

\section{Encoding of I-type subbands}
After the the $T$-levels MCDWT transform of a sequence of
decompositions, $T+1$ temporal subbands are generated, and the
temporal subband $L^{T+1}$ has a one $H$ I-type subband for each
GOP. Notice that the redundancy exploited in the I-type $H$ subbands
is spatial.

To compress the $a.H$ subband in the last iteration of the butterfly,
the following algorithm can be used:
\begin{enumerate}
\item [1.] Compute the 2D-DWT of the $a.L$ subband
\begin{equation}
  LL^2, LH^2, HL^2, HH^2 = \text{DWT}(LL). 
\end{equation}

\item [2.] Desplace 2 bits to the left the $H$ coefficients to create
  space for encoding the subband
  \begin{equation}
    \begin{array}{l}
      LH^2 <<= 2 \\
      HL^2 <<= 2 \\
      HH^2 <<= 2.
    \end{array}
  \end{equation}
\item [3.] Encode de subband:
  \begin{equation}
    \begin{array}{l}
      HL^2 += 1 \\
      HH^2 += 2.
    \end{array}
  \end{equation}
\item [4.] Create an array with the 3 matrices
  \begin{equation}
    H = [LH^2, HL^2, HH^2].
  \end{equation}
\item [5.] Create a linear array with the 3 flattened matrices
  \begin{equation}
    H'=\text{ravel}(H).
  \end{equation}
\item [6.] Find the indices for the $LH^2$, $HL^2$ and $HH^2$ matrices
  that sort $H'$ in descending order by energy
  ($\text{width}=LH^2.\text{shape}[0]$ and $\text{height}=LH^2.\text{shape}[1]$)
  \begin{equation}
    \text{indices} =
    \text{unravel\_index}(\text{argsort}(\text{abs}(H')), \text{width},
    \text{height}).
  \end{equation}
\item [7.] Go over $H'$ sending the wavelet coefficients of subbands
  $LH^2$, $HL^2$ and $HH^2$ in descending order by energy
  \begin{equation}
    \begin{array}{l}
      \text{for~}d\text{~in range}(3): \\
      ~ \text{for~}y\text{~in range}(\text{height}): \\
      ~~ \text{for~}x\text{~in range}(\text{width}): \\
      ~~~ i = 3\times(\text{height}\times \text{width})+y\times \text{width} + x \\
      ~~~ \text{send}(H[H'[\text{indice}[i]] \text{~mod~} 4][H'[\text{indice}[i]] >> 2]).
    \end{array}
  \end{equation}
\end{enumerate}

\end{comment}

\begin{comment}

\section{Progressive reconstruction}
The forward bufferfly should reduce the energy of subband $b.H$$ after
substracting a prediction (see Eq.~\ref{eq:weighted_prediction}) which
is generated using the subbands $a.H$ and $c.H$ as references. If the
prediction is good, the energy of $\tilde{b.H}$ will be small and
viceversa. If the prediction is good, the quantization of the subbands
$a.H$, $b.H$ and $c.H$ should leave more energy in the subbands $a.H$
and $c.H$ than in $b.H$. Therefore, if the prediction is good, the
progressive reconstruction of the subbands (using a progressively
small quantization step) should reconstruct first the information of
$a.H$ and $c.H$.

\section{Coefficients encoding}
Subband $\tilde{b.H}$ is not used any more as a reference and
therefore, it can be compressed. The compression algorithm sort the
coefficients of $\tilde{b.H}$ by the amplitude of the coefficients of
$\tilde{b.L}$, considering that the prediction error will affect in
the same way the low and the high frequencies of $b$. Then, the sorted
coefficients are DPCM encoded, quantized, and entropy encoded.

\section{Multilevel MCLT}
The distance between images $a$, $b$ and $c$ should decrease the
efficiency of the predictions, generating more energy in the
low-frequency subbands. Therefore, the quantization when $T>1$ should
transmit more information of the low-frequency subbands than of the
high-frequency ones when the subbands are quantized.

\end{comment}
